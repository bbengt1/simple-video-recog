# Story 3.8: Performance Metrics Collection and Logging

## Status
Approved

## Story
**As a** developer,
**I want** to collect and log system performance metrics,
**so that** I can monitor system health and validate NFR performance targets.

## Acceptance Criteria
1. Metrics collector module (core/metrics.py) implements MetricsCollector class with collect() method
2. Metrics collected (per NFR21, FR24-26):
   - frames_processed: total count
   - motion_detected: count and hit rate percentage
   - events_created: count
   - events_suppressed: count (de-duplication)
   - coreml_inference_time: average, min, max, p95
   - llm_inference_time: average, min, max, p95
   - cpu_usage: current and average (via psutil)
   - memory_usage: current in MB/GB (via psutil)
   - frame_processing_latency: end-to-end time from motion → event logged
   - system_availability: uptime percentage
3. Metrics stored in-memory with rolling window (last 1000 events for averages)
4. collect() returns MetricsSnapshot object with all current values
5. Metrics logged to dedicated file: logs/metrics.json (JSON Lines format)
6. Metrics logged periodically: Every 60 seconds (configurable via metrics_interval)
7. Metrics included in runtime status display (console output every 60s)
8. Performance: Metrics collection overhead <1% CPU (verified via profiling)
9. Unit tests verify: metric calculation, rolling window, percentile calculation
10. Integration test: Run system for 10 minutes, verify metrics logged correctly and match expected values

## Tasks / Subtasks
- [ ] Task 1: Create MetricsCollector class in core/metrics.py (AC: 1)
  - [ ] Implement __init__ method with SystemConfig dependency injection
  - [ ] Add collect() method signature returning MetricsSnapshot
  - [ ] Import required modules: psutil, numpy, collections, time
- [ ] Task 2: Implement MetricsSnapshot data structure (AC: 4)
  - [ ] Create MetricsSnapshot Pydantic model with all required fields
  - [ ] Add proper type hints and field validations
  - [ ] Include timestamp, processing stats, performance metrics, and system resources
- [ ] Task 3: Implement counter and timing tracking methods (AC: 2)
  - [ ] Add increment_counter(metric_name) for event/frame counting
  - [ ] Add record_inference_time(component, time_ms) for CoreML/LLM timing
  - [ ] Add record_frame_latency(time_ms) for end-to-end processing time
  - [ ] Implement rolling window storage (last 1000 events)
- [ ] Task 4: Implement system resource monitoring (AC: 2)
  - [ ] Add CPU usage monitoring using psutil.cpu_percent()
  - [ ] Add memory usage monitoring using psutil.virtual_memory()
  - [ ] Add system uptime calculation using time module
  - [ ] Convert units appropriately (MB for memory, percentage for CPU)
- [ ] Task 5: Implement percentile calculations (AC: 2, 3)
  - [ ] Use numpy.percentile() for p95 calculations on inference times
  - [ ] Calculate averages from rolling window data
  - [ ] Handle edge cases (empty data, single measurements)
  - [ ] Implement min/max tracking alongside percentiles
- [ ] Task 6: Implement periodic logging (AC: 5, 6)
  - [ ] Add log_metrics() method for JSON Lines file writing
  - [ ] Create logs/metrics.json file with proper directory creation
  - [ ] Implement periodic collection every metrics_interval seconds
  - [ ] Use atomic writes (temp file + rename) for reliability
- [ ] Task 7: Integrate with runtime status display (AC: 7)
  - [ ] Add get_status_display() method for console output formatting
  - [ ] Format metrics for human-readable display ("CPU: 45.2%, Mem: 1.8GB")
  - [ ] Ensure thread-safe access to metrics data
- [ ] Task 8: Implement performance overhead monitoring (AC: 8)
  - [ ] Add timing measurements around collect() method
  - [ ] Calculate CPU overhead percentage during collection
  - [ ] Log warnings if overhead exceeds 1% target
  - [ ] Include overhead metrics in status display
- [ ] Task 9: Create comprehensive unit tests (AC: 9)
  - [ ] Test counter increment and rolling window behavior
  - [ ] Test percentile calculations with various data sets
  - [ ] Test system resource monitoring (mock psutil)
  - [ ] Test MetricsSnapshot serialization and validation
  - [ ] Test edge cases (empty data, single measurements)
- [ ] Task 10: Create integration test (AC: 10)
  - [ ] Run system for 10 minutes with simulated load
  - [ ] Verify metrics.json file creation and JSON Lines format
  - [ ] Validate metric values against expected ranges
  - [ ] Test periodic logging behavior and timing accuracy

## Dev Notes

### Previous Story Insights
From processing pipeline stories: Event counting and timing measurements established. System resource monitoring patterns from storage monitor. File logging patterns from JSON/plaintext loggers.

### Data Models
- **MetricsSnapshot**: Pydantic model containing all performance metrics [Source: architecture/metricssnapshot.md]
- **SystemConfig**: Contains metrics_interval parameter for logging frequency [Source: architecture/systemconfig.md]
- **Rolling Window**: Last 1000 events stored in-memory for percentile calculations [Source: epic-3-event-persistence-data-management.md AC: 3]

### File Locations
- **Module Location**: core/metrics.py [Source: architecture/metrics-collector.md]
- **Class Name**: MetricsCollector [Source: architecture/metrics-collector.md]
- **Metrics File**: logs/metrics.json (JSON Lines format) [Source: architecture/metrics-collector.md]
- **Dependencies**: psutil for system monitoring, numpy for percentiles [Source: architecture/metrics-collector.md]

### Technical Constraints
- **Collection Frequency**: Every 60 seconds (configurable via metrics_interval) [Source: epic-3-event-persistence-data-management.md AC: 6]
- **Rolling Window**: Last 1000 events for averages and percentiles [Source: epic-3-event-persistence-data-management.md AC: 3]
- **Performance Overhead**: <1% CPU during collection [Source: epic-3-event-persistence-data-management.md AC: 8]
- **File Format**: JSON Lines (one MetricsSnapshot per line) [Source: epic-3-event-persistence-data-management.md AC: 5]
- **Percentiles**: p95 calculated using numpy for inference times [Source: epic-3-event-persistence-data-management.md AC: 2]

### Testing Requirements
- **Test Location**: tests/unit/test_metrics.py [Source: architecture/test-organization.md]
- **Test Framework**: pytest with standard assertions [Source: architecture/test-organization.md]
- **Coverage Target**: ≥70% including edge cases [Source: architecture/test-organization.md]
- **Mock Strategy**: Mock psutil for system monitoring, mock time for uptime [Source: architecture/test-organization.md]
- **Integration Test**: tests/integration/test_metrics_integration.py [Source: architecture/test-organization.md]

### Project Structure Notes
- Follows core/ module organization for platform-independent components [Source: architecture/repository-structure.md]
- Metrics collector is a system monitoring component alongside storage monitor [Source: architecture/repository-structure.md]
- No structural conflicts identified between epic requirements and architecture patterns

## Testing

### Testing Standards
- **Test File Location**: tests/unit/test_metrics.py for unit tests, tests/integration/test_metrics_integration.py for integration tests [Source: architecture/test-organization.md]
- **Test Framework**: pytest 7.4+ with pytest-cov for coverage reporting [Source: architecture/test-organization.md]
- **Coverage Requirements**: Target ≥70% coverage across core/metrics.py [Source: architecture/test-organization.md]
- **Mocking Strategy**: Use pytest-mock for psutil and time modules, create mock metric data for testing [Source: architecture/test-organization.md]
- **Test Patterns**: Arrange-Act-Assert pattern, test both success and error scenarios [Source: architecture/unit-test-best-practices.md]
- **Performance Testing**: Include timing assertions to validate <1% CPU overhead [Source: architecture/performance-test-examples.md]

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-10 | 1.0 | Initial story draft created from Epic 3 requirements | SM Agent (Bob) |
| 2025-11-10 | 1.1 | Story approved after checklist validation (10/10) | SM Agent (Bob) |

## Dev Agent Record

### Agent Model Used
*To be populated by development agent*

### Debug Log References
*To be populated by development agent*

### Completion Notes List
*To be populated by development agent*

### File List
*To be populated by development agent*

## QA Results
*To be populated by QA agent*