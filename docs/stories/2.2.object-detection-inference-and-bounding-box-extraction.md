# Story 2.2: Object Detection Inference and Bounding Box Extraction

## Status
Approved

## Story
**As a** developer,
**I want** to run object detection inference on frames and extract detected objects with bounding boxes,
**so that** I know what objects are present and where they are located in the frame.

## Acceptance Criteria

1. CoreMLDetector.detect_objects(frame) method accepts numpy array frame (BGR format from OpenCV)
2. Frame preprocessing: Convert BGR → RGB, resize to model input size (e.g., 416x416), normalize pixel values
3. CoreML inference executed via model.predict() returning raw model output (bounding boxes, class probabilities, confidence scores)
4. Post-processing: Parse model output into list of detected objects, each containing:
   - label: str (e.g., "person", "car", "dog")
   - confidence: float (0.0 to 1.0)
   - bbox: tuple (x, y, width, height) in original frame coordinates
5. Confidence threshold filtering: Only return objects with confidence >= min_confidence (from config, default: 0.5)
6. Non-maximum suppression (NMS) applied to remove duplicate detections of same object
7. detect_objects() returns list[DetectedObject] with proper Pydantic validation
8. Performance requirement: Inference completes in <100ms on M1 Neural Engine (measured and logged)
9. If inference time exceeds 100ms, log WARNING with actual time
10. Unit tests verify: detection on test images with known objects, confidence filtering works, bbox coordinates are correct
11. Integration test: Run detection on sample video frames, verify objects detected match ground truth annotations
12. Performance test: Run 100 inferences, verify average time <100ms and 95th percentile <120ms

## Tasks / Subtasks

- [ ] **Task 1: Create DetectedObject and BoundingBox models** (AC: 4, 7)
  - [ ] Create core/models.py with BoundingBox and DetectedObject Pydantic models
  - [ ] Implement proper field validation (bbox coordinates >= 0, confidence 0.0-1.0)
  - [ ] Add JSON schema examples and documentation
  - [ ] Import models in apple_platform/coreml_detector.py

- [ ] **Task 2: Implement frame preprocessing** (AC: 1, 2)
  - [ ] Add detect_objects(frame: np.ndarray) method to CoreMLDetector
  - [ ] Implement BGR to RGB conversion using cv2.cvtColor
  - [ ] Add frame resizing to match model input dimensions
  - [ ] Implement pixel value normalization (0-255 → 0.0-1.0 if required by model)

- [ ] **Task 3: Execute CoreML inference** (AC: 3)
  - [ ] Call self.model.predict() with preprocessed frame
  - [ ] Handle model input format (likely {'input': frame_array})
  - [ ] Extract raw model outputs (bounding boxes, classes, scores)
  - [ ] Add error handling for inference failures

- [ ] **Task 4: Implement post-processing and NMS** (AC: 4, 6)
  - [ ] Parse raw model outputs into intermediate detection format
  - [ ] Implement Non-Maximum Suppression algorithm to remove duplicates
  - [ ] Convert model coordinates back to original frame coordinates
  - [ ] Create DetectedObject instances for each valid detection

- [ ] **Task 5: Add confidence filtering** (AC: 5)
  - [ ] Filter detections by min_object_confidence from SystemConfig
  - [ ] Log filtered objects at DEBUG level
  - [ ] Return only objects meeting confidence threshold

- [ ] **Task 6: Add performance monitoring** (AC: 8, 9)
  - [ ] Measure inference time using time.perf_counter()
  - [ ] Log inference time at INFO level for each detection
  - [ ] Log WARNING if inference exceeds 100ms threshold
  - [ ] Include timing in returned metadata

- [ ] **Task 7: Create comprehensive unit tests** (AC: 10)
  - [ ] Create tests/unit/test_coreml_detector_inference.py
  - [ ] Test frame preprocessing (BGR→RGB conversion, resizing)
  - [ ] Mock CoreML model.predict() with known outputs
  - [ ] Test confidence filtering and NMS logic
  - [ ] Test bbox coordinate conversion accuracy
  - [ ] Test error handling for inference failures

- [ ] **Task 8: Create integration and performance tests** (AC: 11, 12)
  - [ ] Create tests/integration/test_coreml_inference.py
  - [ ] Test with real CoreML model (conditional on model availability)
  - [ ] Verify detection accuracy on sample frames
  - [ ] Implement performance test with 100 inferences
  - [ ] Validate timing requirements (<100ms average, <120ms 95th percentile)

## Dev Notes

### Previous Story Insights

From Story 2.1 (CoreML Model Loading):
- CoreMLDetector class established with load_model() method
- Model metadata extraction and ANE compatibility validation implemented
- Warm-up inference and timing measurement in place
- Comprehensive unit and integration tests for model loading
- apple_platform/coreml_detector.py contains the detector implementation

### Data Models

**DetectedObject Model:**
- Pydantic model for representing detected objects with bounding boxes
- Fields: label (str), confidence (float), bbox (BoundingBox)
- Used for object detection results throughout the system
- [Source: docs/architecture/detectedobject.md]

**BoundingBox Model:**
- Pydantic model for bounding box coordinates
- Fields: x, y, width, height (all int, >= 0)
- Represents pixel coordinates in original frame
- [Source: docs/architecture/detectedobject.md]

**SystemConfig Integration:**
- min_object_confidence: float field for detection threshold (default: 0.5)
- Located in core/config.py, uses Pydantic for validation
- Configuration loaded from YAML files in config/ directory
- [Source: docs/architecture/systemconfig.md]

### API Specifications

**CoreML Object Detector Interface:**
- `detect_objects(frame: np.ndarray) -> list[DetectedObject]`: Run inference, return detected objects
- `load_model(model_path: str) -> None`: Load CoreML model from file
- `is_model_loaded() -> bool`: Check if model is ready for inference
- Module path: `apple_platform/coreml_detector.py`, Class: `CoreMLObjectDetector`
- [Source: docs/architecture/coreml-object-detector.md]

### Component Specifications

**Component Architecture Pattern:**
- Functional component pattern with clear responsibilities
- Dependencies explicitly declared at module level
- Error handling through custom exceptions
- Logging integrated for observability
- [Source: docs/architecture/component-architecture.md]

**Technology Stack:**
- Python 3.10+, CoreML Tools 7.0+, Vision framework (via coremltools)
- Apple Neural Engine for <100ms inference target
- OpenCV for frame preprocessing (BGR to RGB conversion)
- [Source: docs/architecture/technology-stack-table.md]

### File Locations

**Repository Structure:**
- apple_platform/: Apple Silicon-specific implementations (CoreML, Neural Engine)
- core/: Platform-independent business logic
- integrations/: External service clients
- tests/: Test organization mirroring source structure
- [Source: docs/architecture/repository-structure.md]

**Specific File Paths:**
- CoreML detector: apple_platform/coreml_detector.py (extend existing)
- Data models: core/models.py (new file for DetectedObject, BoundingBox)
- Unit tests: tests/unit/test_coreml_detector_inference.py (new)
- Integration tests: tests/integration/test_coreml_inference.py (new)
- Configuration: core/config.py (min_object_confidence already exists)

### Testing Requirements

**Testing Pyramid Distribution:**
- Unit Tests (70%): Fast, isolated tests with mocked CoreML dependencies
- Integration Tests (25%): Tests with real CoreML models on actual hardware
- E2E Tests (5%): Deferred to Phase 3
- [Source: docs/architecture/testing-pyramid.md]

**Unit Test Best Practices:**
- Use pytest fixtures for common test data (extend conftest.py)
- Mock external dependencies (CoreML models, file system)
- Test edge cases and error conditions
- Parametrize tests for multiple scenarios
- [Source: docs/architecture/unit-test-best-practices.md]

**Test Coverage Requirements:**
- CoreML detector inference: ≥80% coverage
- All public methods tested with success and error paths
- Edge cases: empty frames, no detections, low confidence objects
- Performance assertions for inference timing
- [Source: docs/architecture/test-coverage-requirements.md]

### Technical Constraints

**Critical Fullstack Rules:**
- Use dependency injection for testability (SystemConfig injection)
- Mock external dependencies appropriately (CoreML models in unit tests)
- Test error handling and edge cases (inference failures, invalid frames)
- Follow established import patterns
- [Source: docs/architecture/critical-fullstack-rules.md]

**Python Code Style:**
- PEP 8 compliance with 100-character line length
- Type hints for all function parameters and return values
- Google-style docstrings for all public functions and classes
- Proper import organization (stdlib, third-party, local)
- [Source: docs/architecture/python-code-style.md]

**Performance Requirements:**
- Inference time: <100ms per frame (NFR requirement)
- Memory usage: CoreML inference should not cause excessive memory consumption
- Frame preprocessing: Should complete quickly (<10ms)
- [Source: docs/architecture/performance-optimization.md]

**Error Handling Standards:**
- Custom exceptions for domain-specific errors (extend CoreMLLoadError if needed)
- Descriptive error messages for troubleshooting
- Logging at appropriate levels (INFO for success, WARNING for issues, ERROR for failures)
- [Source: docs/architecture/error-handling-standards.md]

## Testing

### Test Organization

**Unit Tests:** tests/unit/test_coreml_detector_inference.py
- Mock CoreML models and inference results
- Test all preprocessing, post-processing, and filtering logic
- Fast execution for development feedback

**Integration Tests:** tests/integration/test_coreml_inference.py
- Test with real CoreML models on Apple Silicon hardware
- Validate detection accuracy and performance
- Measure actual inference times and memory usage

**Performance Tests:** tests/integration/test_coreml_inference.py
- Run 100 inferences to validate timing requirements
- Statistical analysis of inference times (average, 95th percentile)

**Test Fixtures:**
- Extend conftest.py with mock_detections fixture
- Use sample_config for configuration testing
- Mock frame data for preprocessing tests

### Test Coverage Requirements

- CoreML detector inference methods: ≥80% coverage
- All preprocessing steps tested with various frame formats
- Post-processing logic tested with different model outputs
- Edge cases: empty detections, single object, multiple overlapping objects
- Performance: Timing measurements validated against requirements

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-09 | 1.2 | Story approved for development | Sarah (Product Owner) |
| 2025-11-09 | 1.1 | Story ready for review | Bob (Scrum Master) |
| 2025-11-09 | 1.0 | Initial story draft created | Bob (Scrum Master) |

## Dev Agent Record
_To be filled by Dev Agent_

## PO Validation
**Validation Date:** November 9, 2025
**Product Owner:** Sarah
**Validation Decision:** APPROVED

### Story Quality Assessment
**Business Value:** High - Core functionality for object detection, essential for Epic 2 intelligence layer
**Technical Feasibility:** High - Builds directly on Story 2.1 foundation, uses established patterns
**Acceptance Criteria:** Clear and measurable - 12 specific, testable ACs covering functionality and performance
**Dependencies:** Properly identified - depends on Story 2.1 (already completed)
**Risk Level:** Low - Well-defined scope, comprehensive testing strategy, performance requirements specified

### Epic 2 Alignment
- **Strategic Fit:** Directly advances Epic 2 goal of object detection and semantic understanding
- **Vertical Slice:** Complete functionality from frame input to detected objects output
- **Value Delivery:** Enables downstream LLM integration and event generation
- **Dependencies:** Story 2.1 foundation is solid and completed

### Acceptance Criteria Review
| AC # | Description | Assessment | Rationale |
|------|-------------|------------|-----------|
| 1-3 | Core detection pipeline | ✅ APPROVED | Clear input/output specifications, preprocessing well-defined |
| 4 | Post-processing & NMS | ✅ APPROVED | Comprehensive object extraction with proper data structures |
| 5 | Confidence filtering | ✅ APPROVED | Uses existing config parameter, proper threshold logic |
| 6 | NMS implementation | ✅ APPROVED | Critical for accuracy, prevents duplicate detections |
| 7 | Pydantic validation | ✅ APPROVED | Ensures data integrity, follows established patterns |
| 8-9 | Performance requirements | ✅ APPROVED | <100ms target aligns with NFRs, proper monitoring |
| 10-12 | Testing coverage | ✅ APPROVED | Comprehensive testing pyramid with performance validation |

### Recommendations
- **Approved for Development:** Story meets all quality standards and business requirements
- **Priority:** High - Critical path for Epic 2 completion
- **Timeline Estimate:** 2-3 development cycles based on complexity
- **Success Metrics:** All ACs satisfied, performance targets met, comprehensive test coverage

### Business Impact
This story delivers the core object detection capability that transforms the system from basic motion detection to intelligent object recognition. The <100ms performance requirement ensures real-time processing capability, and the comprehensive testing approach guarantees reliability for production deployment.

## QA Results
_To be filled by QA Agent_

## Story Draft Checklist Results

### 1. Goal & Context Clarity
- [x] Story goal/purpose is clearly stated
- [x] Relationship to epic goals is evident
- [x] How the story fits into overall system flow is explained
- [x] Dependencies on previous stories are identified (if applicable)
- [x] Business context and value are clear

### 2. Technical Implementation Guidance
- [x] Key files to create/modify are identified (not necessarily exhaustive)
- [x] Technologies specifically needed for this story are mentioned
- [x] Critical APIs or interfaces are sufficiently described
- [x] Necessary data models or structures are referenced
- [x] Required environment variables are listed (if applicable)
- [x] Any exceptions to standard coding patterns are noted

### 3. Reference Effectiveness
- [x] References to external documents point to specific relevant sections
- [x] Critical information from previous stories is summarized (not just referenced)
- [x] Context is provided for why references are relevant
- [x] References use consistent format (e.g., `docs/filename.md#section`)

### 4. Self-Containment Assessment
- [x] Core information needed is included (not overly reliant on external docs)
- [x] Implicit assumptions are made explicit
- [x] Domain-specific terms or concepts are explained
- [x] Edge cases or error scenarios are addressed

### 5. Testing Guidance
- [x] Required testing approach is outlined
- [x] Key test scenarios are identified
- [x] Success criteria are defined
- [x] Special testing considerations are noted (if applicable)

**Final Assessment: READY**

The story provides comprehensive context for implementation with clear goals, detailed technical guidance, effective references, and strong self-containment. All acceptance criteria are specific and testable. The developer agent has sufficient information to implement without significant additional research.</content>
</xai:function_call">The story document has been created successfully. The story is now ready for validation and approval by the Product Owner.