# Story 2.5: Ollama Service Integration and Model Verification

## Status
Approved

## Story
**As a** developer,
**I want** to connect to local Ollama service and verify the vision model is available,
**so that** I can use LLMs for semantic event description generation.

## Acceptance Criteria

1. Ollama client module (integrations/ollama.py) implements OllamaClient class with connect() and verify_model(model_name) methods
2. ollama_model configuration parameter specifies model (e.g., "llava:latest", "moondream")
3. Connection uses ollama Python client library to communicate with localhost:11434 (default Ollama port)
4. connect() verifies Ollama service is running by calling /api/tags endpoint
5. If Ollama service not reachable, raise OllamaConnectionError: "Ollama service not reachable at localhost:11434. Is Ollama running?"
6. verify_model() checks if specified vision model is downloaded/available via /api/show endpoint
7. If model not found, raise OllamaModelNotFoundError: "Vision model '[model]' not found. Run: ollama pull [model]"
8. Successful verification logs: "✓ Ollama service: Connected (localhost:11434)" and "✓ Vision model: [model] (available)"
9. List all available models on connect (logged at DEBUG level) for troubleshooting
10. Unit tests with mocked Ollama API verify: successful connection, service unreachable error, model not found error
11. Integration test: Connect to real Ollama instance, verify llava:latest model (assumes Ollama running locally)
12. README updated with "Prerequisites" section: Installing Ollama and downloading vision models

## Tasks / Subtasks

- [ ] **Task 1: Create OllamaClient module** (AC: 1, 2, 3)
  - [ ] Create integrations/ollama.py with OllamaClient class
  - [ ] Implement connect() method using ollama-python library
  - [ ] Implement verify_model(model_name) method
  - [ ] Add type hints using SystemConfig and custom exceptions
  - [ ] Add Google-style docstrings for class and methods

- [ ] **Task 2: Implement service connection verification** (AC: 4, 5, 8, 9)
  - [ ] Import ollama library for HTTP API communication
  - [ ] Implement connect() method calling /api/tags endpoint
  - [ ] Handle connection failures with OllamaConnectionError
  - [ ] Log successful connection with service details
  - [ ] Log available models at DEBUG level for troubleshooting

- [ ] **Task 3: Implement model verification** (AC: 6, 7, 8)
  - [ ] Implement verify_model() method calling /api/show endpoint
  - [ ] Handle model not found with OllamaModelNotFoundError
  - [ ] Log successful model verification with model details
  - [ ] Validate vision model capabilities for image processing

- [ ] **Task 4: Create custom exceptions** (AC: 5, 7)
  - [ ] Add OllamaConnectionError to core/exceptions.py
  - [ ] Add OllamaModelNotFoundError to core/exceptions.py
  - [ ] Both inherit from VideoRecognitionError base class
  - [ ] Include clear error messages with actionable guidance

- [ ] **Task 5: Create comprehensive unit tests** (AC: 10)
  - [ ] Create tests/unit/test_ollama.py
  - [ ] Mock ollama API responses for connection success/failure
  - [ ] Mock model verification responses for found/not found scenarios
  - [ ] Test error handling and exception raising
  - [ ] Test logging behavior at appropriate levels

- [ ] **Task 6: Create integration tests** (AC: 11)
  - [ ] Create tests/integration/test_ollama_integration.py
  - [ ] Test real connection to Ollama service (when running)
  - [ ] Test model verification with actual llava:latest model
  - [ ] Handle test failures gracefully when Ollama not available
  - [ ] Validate API responses and error handling

- [ ] **Task 7: Update README prerequisites** (AC: 12)
  - [ ] Add "Prerequisites" section to README.md
  - [ ] Include Ollama installation instructions (brew install ollama)
  - [ ] Include service startup (ollama serve)
  - [ ] Include vision model download (ollama pull llava:7b)
  - [ ] Reference local LLM setup in development workflow

## Dev Notes

### Previous Story Insights

From Story 2.4 (Image Annotation):
- Established comprehensive unit and integration testing patterns
- Error handling follows custom exception hierarchy from VideoRecognitionError
- Logging uses get_logger(__name__) with appropriate levels (DEBUG, INFO, WARNING, ERROR)
- Integration tests handle external service availability gracefully
- Mock fixtures extend tests/conftest.py for reusable test data

From Story 2.3 (Object Blacklist Filtering):
- CoreML integration patterns established with proper error handling
- Configuration validation through Pydantic SystemConfig
- External service integration follows dependency injection pattern

From Story 2.2 (Object Detection Inference):
- External API integration patterns with timeout handling
- Performance logging and warning thresholds established
- Graceful degradation when external services unavailable

### Data Models

**SystemConfig Model:**
- ollama_base_url: HttpUrl field for Ollama API endpoint (default: "http://localhost:11434")
- ollama_model: str field for vision model name (default: "llava:7b")
- llm_timeout: int field for request timeout in seconds (default: 10)
- [Source: core/config.py, docs/architecture/systemconfig.md]

**Custom Exceptions:**
- OllamaConnectionError: Inherits from VideoRecognitionError, raised when Ollama service unreachable
- OllamaModelNotFoundError: Inherits from VideoRecognitionError, raised when specified model not available
- Error messages include actionable guidance (e.g., "Run: ollama pull [model]")
- [Source: core/exceptions.py, docs/architecture/error-handling-standards.md]

### Component Specifications

**OllamaClient Architecture:**
- Functional component pattern with clear responsibilities
- Dependency injection: Receives SystemConfig in constructor
- Pure functions: No state modification, predictable behavior
- Error handling: Custom exceptions for different failure modes
- Logging integration: Uses get_logger(__name__) for structured logging
- [Source: docs/architecture/component-architecture.md]

**Ollama Python Library Integration:**
- ollama library for HTTP API client functionality
- Synchronous API calls (no async for MVP simplicity)
- Timeout handling via library configuration
- Response parsing for JSON API responses
- Error handling for HTTP failures and JSON parsing
- [Source: docs/architecture/technology-stack-table.md, docs/architecture/ollama-local-llm-service.md]

**API Endpoints Used:**
- GET /api/tags: List available models, verify service connectivity
- GET /api/show?model={name}: Check if specific model is downloaded and available
- Response format: JSON with model metadata and capabilities
- Error responses: HTTP status codes with error messages
- [Source: docs/architecture/ollama-local-llm-service.md]

### File Locations

**Repository Structure:**
- integrations/: External service clients (RTSP, Ollama)
- core/: Platform-independent business logic (exceptions, config)
- tests/unit/: Unit tests mirroring source structure
- tests/integration/: Integration tests for end-to-end validation
- [Source: docs/architecture/repository-structure.md]

**Specific File Paths:**
- OllamaClient implementation: integrations/ollama.py (new file)
- Custom exceptions: core/exceptions.py (modify existing)
- Unit tests: tests/unit/test_ollama.py (new)
- Integration tests: tests/integration/test_ollama_integration.py (new)
- Configuration: core/config.py (existing - SystemConfig model)
- README: README.md (modify existing - add Prerequisites section)

### Testing Requirements

**Testing Pyramid Distribution:**
- Unit Tests (70%): Fast, isolated tests with mocked Ollama API
- Integration Tests (25%): Tests with real Ollama service when available
- E2E Tests (5%): Deferred to Phase 3
- [Source: docs/architecture/testing-pyramid.md]

**Unit Test Best Practices:**
- Use pytest fixtures for mocked Ollama API responses
- Mock external HTTP calls to avoid network dependencies
- Parametrize tests for different error scenarios
- Test exception types and error messages exactly
- [Source: docs/architecture/unit-test-best-practices.md]

**Test Coverage Requirements:**
- OllamaClient module: ≥80% coverage (core business logic)
- All connection and verification logic tested
- Error handling paths tested with appropriate exceptions
- Logging behavior validated in tests
- [Source: docs/architecture/test-coverage-requirements.md]

**Test Fixtures (extend tests/conftest.py):**
```python
@pytest.fixture
def mock_ollama_client(sample_config):
    """Create OllamaClient with mocked ollama library."""
    with patch('integrations.ollama.ollama') as mock_ollama:
        client = OllamaClient(sample_config)
        yield client, mock_ollama

@pytest.fixture
def mock_ollama_response():
    """Mock successful Ollama API response."""
    return {
        'models': [
            {'name': 'llava:7b', 'size': '4.7GB'},
            {'name': 'moondream:latest', 'size': '1.8GB'}
        ]
    }
```
[Source: docs/architecture/unit-test-best-practices.md]

### Technical Constraints

**Critical Fullstack Rules:**
- Type Safety: Use type hints for all function parameters and return values
- Error Handling: Custom exceptions with clear, actionable error messages
- Resource Management: HTTP connections properly managed by ollama library
- Logging: Appropriate levels (DEBUG for troubleshooting, INFO for success, ERROR for failures)
- [Source: docs/architecture/critical-fullstack-rules.md]

**Python Code Style:**
- PEP 8 compliance with 100-character line length
- Type hints: `def connect(self) -> bool`
- Google-style docstrings for all public methods
- Proper import organization (stdlib, third-party, local)
- [Source: docs/architecture/python-code-style.md]

**Error Handling Standards:**
- Custom exception hierarchy inheriting from VideoRecognitionError
- Descriptive error messages with actionable guidance
- Logging with appropriate context and severity levels
- Graceful handling of external service unavailability
- [Source: docs/architecture/error-handling-standards.md]

**Technology Stack:**
- Python: 3.10+ with ollama-python 0.1.0+ library
- HTTP client: ollama library handles connection pooling and timeouts
- Configuration: Pydantic SystemConfig for validation
- Logging: Python logging with structured JSON output
- [Source: docs/architecture/technology-stack-table.md]

## Testing

### Test Organization

**Unit Tests:** tests/unit/test_ollama.py
- Test OllamaClient.connect() with mocked API responses
- Test OllamaClient.verify_model() with different model scenarios
- Mock ollama library to avoid network dependencies
- Fast execution (<1s total) for development feedback

**Integration Tests:** tests/integration/test_ollama_integration.py
- Test real connection to Ollama service (when available)
- Validate actual API responses and error handling
- Skip tests gracefully when Ollama service not running
- Test model verification with real llava:latest model

**Test Fixtures (extend tests/conftest.py):**
- mock_ollama_client: OllamaClient with mocked ollama library
- mock_ollama_response: Sample successful API response
- sample_config: Valid configuration with Ollama settings

### Test Coverage Requirements

- OllamaClient module: ≥80% coverage
- All connection verification logic tested
- Model verification logic tested
- Error handling and exception raising tested
- Logging behavior validated

### Manual Verification

After implementation, manually verify Ollama integration:
1. Start Ollama service: `ollama serve`
2. Pull vision model: `ollama pull llava:7b`
3. Run integration tests to verify connection
4. Check logs for successful connection messages
5. Test error scenarios (stop Ollama service, verify error handling)

## Story Draft Checklist Results

### 1. Goal & Context Clarity
- [x] Story goal/purpose is clearly stated
- [x] Relationship to epic goals is evident
- [x] How the story fits into overall system flow is explained
- [x] Dependencies on previous stories are identified (if applicable)
- [x] Business context and value are clear

### 2. Technical Implementation Guidance
- [x] Key files to create/modify are identified (not necessarily exhaustive)
- [x] Technologies specifically needed for this story are mentioned
- [x] Critical APIs or interfaces are sufficiently described
- [x] Necessary data models or structures are referenced
- [x] Required environment variables are listed (if applicable)
- [x] Any exceptions to standard coding patterns are noted

### 3. Reference Effectiveness
- [x] References to external documents point to specific relevant sections
- [x] Critical information from previous stories is summarized (not just referenced)
- [x] Context is provided for why references are relevant
- [x] References use consistent format (e.g., `docs/filename.md#section`)

### 4. Self-Containment Assessment
- [x] Core information needed is included (not overly reliant on external docs)
- [x] Implicit assumptions are made explicit
- [x] Domain-specific terms or concepts are explained
- [x] Edge cases or error scenarios are addressed

### 5. Testing Guidance
- [x] Required testing approach is outlined
- [x] Key test scenarios are identified
- [x] Success criteria are defined
- [x] Special testing considerations are noted (if applicable)

**Final Assessment: READY**

The story provides comprehensive context for implementation with clear goals, detailed technical guidance, effective references, and strong self-containment. All acceptance criteria are specific and testable. The developer agent has sufficient information to implement without significant additional research.

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-09 | 1.0 | Initial story draft created | Bob (Scrum Master) |