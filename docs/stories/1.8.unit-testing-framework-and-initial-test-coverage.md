# Story 1.8: Unit Testing Framework and Initial Test Coverage

## Status
Draft

## Story
**As a** developer,
**I want** a pytest-based testing framework with initial unit tests for core logic,
**so that** I can verify functionality and prevent regressions as I add features.

## Acceptance Criteria

1. pytest installed and added to requirements-test.txt (separate from runtime requirements)
2. Test directory structure created: tests/unit/, tests/integration/, tests/conftest.py
3. conftest.py contains shared fixtures: sample_config, mock_rtsp_camera, sample_frame
4. Unit tests for configuration validation (tests/unit/test_config.py): valid config loads, invalid config raises error, missing fields detected, type validation works
5. Unit tests for motion detection (tests/unit/test_motion.py): motion detected in test video, static scenes return False, threshold configuration works, performance <50ms per frame
6. Unit tests for frame sampling (tests/unit/test_sampling.py): sampling rates verified, metrics tracking accurate
7. Unit tests for RTSP client (tests/unit/test_rtsp.py): connection logic, error handling, reconnection backoff (using mocks)
8. Test coverage measurement configured: `pytest --cov=core --cov=integrations --cov-report=term`
9. Coverage target for Epic 1: ≥60% (will increase to 70% by Epic 4 per NFR29)
10. All tests pass: `pytest tests/unit -v` returns zero failures
11. README updated with "Running Tests" section showing pytest commands
12. CI/CD placeholder: .github/workflows/tests.yml created (runs on pull request, executes pytest)

## Tasks / Subtasks

- [ ] **Task 1: Set up pytest testing framework** (AC: 1, 2)
  - [ ] Create requirements-dev.txt with pytest>=7.4, pytest-cov>=4.1
  - [ ] Verify test directory structure exists: tests/unit/, tests/integration/, tests/conftest.py
  - [ ] Ensure pyproject.toml has pytest configuration with coverage settings

- [ ] **Task 2: Enhance shared test fixtures in conftest.py** (AC: 3)
  - [ ] Verify sample_config fixture exists with valid configuration dictionary
  - [ ] Verify mock_frame fixture exists for OpenCV frame testing
  - [ ] Add mock_rtsp_camera fixture for RTSP client testing
  - [ ] Add temp_config_file fixture for file-based config testing

- [ ] **Task 3: Create unit tests for configuration validation** (AC: 4)
  - [ ] Create tests/unit/test_config.py with comprehensive config validation tests
  - [ ] Test valid config loads successfully and returns SystemConfig instance
  - [ ] Test invalid config raises appropriate validation errors
  - [ ] Test missing required fields detected and reported
  - [ ] Test type validation works for all config fields

- [ ] **Task 4: Create unit tests for motion detection** (AC: 5)
  - [ ] Create tests/unit/test_motion.py with motion detection algorithm tests
  - [ ] Test motion detected in frames with actual movement (using mock frames)
  - [ ] Test static scenes return has_motion=False
  - [ ] Test threshold configuration affects detection sensitivity
  - [ ] Test performance requirement: motion detection completes in <50ms per frame

- [ ] **Task 5: Create unit tests for frame sampling** (AC: 6)
  - [ ] Create tests/unit/test_sampling.py with frame sampling logic tests
  - [ ] Test sampling rates work correctly (rate=1 processes all, rate=10 processes every 10th)
  - [ ] Test metrics tracking is accurate for sampled vs total frames
  - [ ] Test sampling applies after motion detection as specified

- [ ] **Task 6: Create unit tests for RTSP client** (AC: 7)
  - [ ] Create tests/unit/test_rtsp.py with RTSP client functionality tests
  - [ ] Test connection logic with mocked VideoCapture
  - [ ] Test error handling for connection failures and invalid URLs
  - [ ] Test reconnection backoff logic with exponential delays
  - [ ] Test frame capture and queue management

- [ ] **Task 7: Configure test coverage measurement** (AC: 8)
  - [ ] Verify pytest-cov is configured in pyproject.toml
  - [ ] Test coverage command works: pytest --cov=core --cov=integrations --cov-report=term
  - [ ] Verify coverage excludes main.py and other non-production code
  - [ ] Generate and verify HTML coverage reports

- [ ] **Task 8: Verify coverage targets and test execution** (AC: 9, 10)
  - [ ] Run full test suite and verify all tests pass
  - [ ] Check coverage meets ≥60% target for Epic 1
  - [ ] Verify core/ modules achieve ≥80% coverage where applicable
  - [ ] Document current coverage levels and gaps

- [ ] **Task 9: Update documentation with testing instructions** (AC: 11)
  - [ ] Add "Running Tests" section to README.md
  - [ ] Include commands for running unit tests, integration tests, and coverage reports
  - [ ] Document pytest commands and coverage report generation
  - [ ] Add examples of test execution with different options

- [ ] **Task 10: Create CI/CD placeholder workflow** (AC: 12)
  - [ ] Create .github/workflows/tests.yml with basic pytest execution
  - [ ] Configure workflow to run on pull requests and pushes to main
  - [ ] Include coverage reporting and failure on coverage below threshold
  - [ ] Add Python version matrix testing (3.10, 3.11, 3.12)

## Dev Notes

### Previous Story Insights

From Story 1.7 (CLI Entry Point):
- Testing framework already partially established with pytest configuration in pyproject.toml
- conftest.py exists with basic fixtures (sample_config, mock_frame, mock_video_capture)
- Integration tests exist and are working (test_main_execution.py)
- Some unit tests already exist (test_config.py, test_health_check.py, test_logging.py, test_motion_detector.py, test_pipeline.py, test_rtsp_client.py)
- Current test coverage is 85% overall, exceeding targets
- [Source: docs/stories/1.7.basic-cli-entry-point-and-execution.md#Dev Agent Record]

### Data Models

**Test Fixtures (from conftest.py):**
- sample_config: Dictionary with valid configuration for all system components
- mock_frame: numpy array representing OpenCV frame (480x640x3)
- mock_video_capture: Mock cv2.VideoCapture for RTSP client testing
- temp_config_file: Temporary YAML file for config loading tests
- [Source: tests/conftest.py]

### API Specifications

**pytest Configuration (from pyproject.toml):**
- testpaths: ["tests"]
- python_files: "test_*.py"
- addopts: "--cov=core --cov=platform --cov=integrations --cov-report=html --cov-report=term"
- [Source: pyproject.toml]

### Component Specifications

**Test Directory Structure:**
- tests/unit/: Unit tests with mocked dependencies
- tests/integration/: Tests with real component interactions
- tests/conftest.py: Shared pytest fixtures
- [Source: docs/architecture/test-organization.md]

**Coverage Requirements:**
- Overall: ≥70% (NFR requirement)
- core/: ≥80% (critical business logic)
- platform/: ≥70% (CoreML integration)
- integrations/: ≥60% (external services)
- Excluded: main.py, scripts/, test files
- [Source: docs/architecture/test-coverage-requirements.md]

### File Locations

**Test Files:**
- Unit tests: tests/unit/test_*.py
- Integration tests: tests/integration/test_*.py
- Shared fixtures: tests/conftest.py
- CI/CD: .github/workflows/tests.yml

**Configuration Files:**
- Test dependencies: requirements-dev.txt
- pytest config: pyproject.toml [tool.pytest.ini_options]
- [Source: docs/architecture/test-organization.md]

### Testing Requirements

**Testing Pyramid Distribution:**
- Unit Tests: 70% (fast, isolated with mocks)
- Integration Tests: 25% (real dependencies)
- E2E Tests: 5% (full workflow, Phase 3)
- [Source: docs/architecture/testing-pyramid.md]

**Unit Test Best Practices:**
- Use pytest fixtures for common test data
- Mock external dependencies (RTSP, file system, external APIs)
- Test edge cases and error conditions
- Parametrize tests for multiple scenarios
- [Source: docs/architecture/unit-test-best-practices.md]

### Technical Constraints

**Critical Fullstack Rules:**
- Use dependency injection for testability
- Mock external dependencies appropriately
- Test error handling and edge cases
- Follow established import patterns

**Python Code Style:**
- Test files follow same standards as production code
- Descriptive test function names (test_*)
- Clear assertions with helpful error messages
- [Source: docs/architecture/python-code-style.md]

**Test Execution:**
- pytest --cov command for coverage reporting
- pytest -v for verbose test output
- HTML reports in htmlcov/ directory
- [Source: docs/architecture/test-coverage-requirements.md]

### Project Structure Notes

**Repository Structure Alignment:**
- Test directory mirrors source structure (unit/, integration/)
- conftest.py in tests/ root for global fixtures
- CI/CD workflows in .github/workflows/
- No structural conflicts identified

[Source: docs/architecture/repository-structure.md]

## Testing

### Test Organization

**Unit Tests:** tests/unit/test_*.py
- Mock external dependencies
- Test individual components in isolation
- Fast execution (<10s total)

**Integration Tests:** tests/integration/test_*.py
- Test component interactions
- Use real dependencies where safe
- Validate end-to-end flows

**Coverage Targets:**
- Overall: ≥70%
- core/: ≥80%
- integrations/: ≥60%

[Source: docs/architecture/testing-pyramid.md, docs/architecture/test-coverage-requirements.md]

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-09 | 1.0 | Initial story draft created with comprehensive technical context | Bob (Scrum Master) |

## Dev Agent Record
_To be filled by Dev Agent_

## QA Results
_To be filled by QA Agent_

## Story Draft Checklist Results

### 1. Goal & Context Clarity
- [x] Story goal/purpose is clearly stated
- [x] Relationship to epic goals is evident
- [x] How the story fits into overall system flow is explained
- [x] Dependencies on previous stories are identified (if applicable)
- [x] Business context and value are clear

### 2. Technical Implementation Guidance
- [x] Key files to create/modify are identified (not necessarily exhaustive)
- [x] Technologies specifically needed for this story are mentioned
- [x] Critical APIs or interfaces are sufficiently described
- [x] Necessary data models or structures are referenced
- [x] Required environment variables are listed (if applicable)
- [x] Any exceptions to standard coding patterns are noted

### 3. Reference Effectiveness
- [x] References to external documents point to specific relevant sections
- [x] Critical information from previous stories is summarized (not just referenced)
- [x] Context is provided for why references are relevant
- [x] References use consistent format (e.g., `docs/filename.md#section`)

### 4. Self-Containment Assessment
- [x] Core information needed is included (not overly reliant on external docs)
- [x] Implicit assumptions are made explicit
- [x] Domain-specific terms or concepts are explained
- [x] Edge cases or error scenarios are addressed

### 5. Testing Guidance
- [x] Required testing approach is outlined
- [x] Key test scenarios are identified
- [x] Success criteria are defined
- [x] Special testing considerations are noted (if applicable)

**Final Assessment: READY**

The story provides comprehensive context for implementation with clear goals, detailed technical guidance, effective references, and strong self-containment. All acceptance criteria are specific and testable. The developer agent has sufficient information to implement without significant additional research.