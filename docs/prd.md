# Local Video Recognition System Product Requirements Document (PRD)

**Version:** 1.0
**Date:** 2025-11-08
**Status:** Draft

---

## Goals and Background Context

### Goals

- Successfully deploy a functional ML/CV system on local M1 hardware demonstrating practical video recognition capabilities
- Create automated awareness of events in monitored spaces through semantic understanding (not just motion detection)
- Build a comprehensive learning laboratory for understanding vision models, Apple Neural Engine optimization, and real-time processing pipelines
- Deliver actionable event alerts with <5% false positive rate through intelligent filtering and semantic analysis
- Produce a well-documented, open-source project demonstrating ML engineering competency for portfolio development
- Maintain 100% local processing with zero cloud dependencies, validating privacy-first architecture

### Background Context

This PRD defines the MVP for a **Local Video Recognition System** that addresses a critical gap in home security and ML learning: the lack of semantic understanding in local-first camera solutions. While cloud services (Nest, Ring) offer smart features, they compromise privacy. Existing open-source alternatives (Frigate, ZoneMinder) provide basic motion/object detection but lack the semantic reasoning ("person carrying package") that vision language models enable.

The project leverages recent advances in lightweight vision LLMs (LLaVA, Moondream) and Apple Silicon's Neural Engine to make real-time local processing feasible. Through a hybrid architecture (motion detection â†’ CoreML filtering â†’ LLM analysis), the system processes <1% of frames with expensive models while maintaining comprehensive awareness. This serves dual purposes: a functional home security system and a hands-on learning platform for ML/CV deployment on edge devices.

### Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-08 | 1.0 | Initial PRD draft from Project Brief | John (PM) |

---

## Requirements

### Functional Requirements

#### Input & Camera Management

**FR1:** The system SHALL connect to a single RTSP camera stream using configurable URL, username, password, and connection parameters defined in YAML configuration

**FR2:** The system SHALL capture video frames from the RTSP stream continuously without requiring manual intervention

#### Motion Detection & Frame Processing

**FR3:** The system SHALL implement motion detection using OpenCV background subtraction to identify frames with significant changes from baseline

**FR4:** The system SHALL filter out static frames where motion detection confidence is below a configurable threshold

**FR5:** The system SHALL support configurable frame sampling rate (e.g., "process every Nth frame") to optimize performance vs. coverage tradeoff

#### Object Detection (CoreML)

**FR6:** The system SHALL perform object detection on motion-triggered frames using CoreML models optimized for Apple Neural Engine

**FR7:** The system SHALL support configurable object blacklisting to exclude specific object types from processing (e.g., "cats", "trees")

#### LLM Integration (Ollama)

**FR8:** The system SHALL integrate with local Ollama instance to perform vision language model inference on filtered frames

**FR9:** The system SHALL support configurable LLM model selection (LLaVA, Moondream, or other Ollama vision models) via YAML configuration

**FR10:** The system SHALL generate semantic event descriptions in structured JSON format containing: timestamp, detected objects (with labels and confidence scores), action description in natural language, and event location/context

#### Logging & Output

**FR11:** The system SHALL log all detected events in JSON format with complete structured data for programmatic access

**FR12:** The system SHALL log all detected events in human-readable plaintext format for easy manual review

**FR13:** The system SHALL save annotated image frames with bounding boxes and labels for all detected objects

**FR14:** The system SHALL support configurable logging levels (DEBUG, INFO, WARNING, ERROR) via YAML configuration to control verbosity of console and file output

#### Configuration Management

**FR15:** The system SHALL load all configuration parameters from a YAML file at startup, including camera settings, model selection, thresholds, frame sampling rate, and storage location

**FR16:** The system SHALL validate YAML configuration against a schema and provide clear error messages for invalid or missing required fields

**FR17:** The system SHALL validate YAML configuration schema at startup and exit with descriptive error messages indicating the specific field, expected format, and provided value for any validation failures

**FR18:** The system SHALL output configuration validation results in structured format showing: each parameter name, expected type/format, actual value provided, validation status (âœ“ valid / âœ— invalid with reason)

**FR19:** The system SHALL include an example YAML configuration file (config.example.yaml) with extensive inline comments documenting all available parameters, expected formats, and recommended default values

**FR20:** The system SHALL support hot-reload of configuration file on SIGHUP signal, reloading configurable parameters (thresholds, sampling rate, logging level, de-duplication window) without restarting the process or reconnecting to camera stream

#### Startup & Initialization

**FR21:** The system SHALL load and initialize CoreML models at startup with validation that the model is compatible with Apple Neural Engine

**FR22:** The system SHALL verify Ollama service availability and selected vision model accessibility at startup before beginning stream processing

**FR23:** The system SHALL display version and build information at startup including: application version, Python version, OpenCV version, CoreML version, and Ollama client version

**FR24:** The system SHALL display a startup health check summary showing: configuration loaded successfully, CoreML model loaded and ANE-compatible, Ollama service reachable, selected vision model available, RTSP connection established, and system ready status

#### Runtime Operations & Monitoring

**FR25:** The system SHALL implement event de-duplication logic to prevent multiple alerts for the same continuous event using time-based suppression windows (configurable threshold)

**FR26:** The system SHALL collect and log performance metrics including: frame processing rate, motion detection hit rate, CoreML inference time, LLM inference time, memory usage, and CPU utilization

**FR27:** The system SHALL display runtime performance metrics to console at configurable intervals (e.g., every 60 seconds) including: frames processed, events detected, average inference times, and current resource usage

#### Failure Handling & Shutdown

**FR28:** The system SHALL gracefully handle RTSP connection failures with error logging and controlled shutdown

**FR29:** The system SHALL detect disk full conditions and perform graceful shutdown with clear error message (hard stop, no recovery attempt)

**FR30:** The system SHALL detect processing failures (motion detection, object recognition, LLM inference) and log stack traces with clear error messages before graceful shutdown

**FR31:** The system SHALL handle graceful shutdown on SIGINT (Ctrl+C) and SIGTERM signals by stopping stream processing, flushing logs, and releasing resources

#### Command Line Interface

**FR32:** The system SHALL provide command-line help information via --help or -h flag displaying: usage syntax, configuration file parameter, dry-run mode flag, version flag, and examples

**FR33:** The system SHALL provide version information via --version or -v flag displaying: application version, build date, Python version, and key dependency versions (OpenCV, CoreML, Ollama client)

**FR34:** The system SHALL support dry-run mode via --dry-run flag that validates configuration file, checks CoreML model compatibility, verifies Ollama connectivity, tests RTSP connection, and exits with success/failure status without starting stream processing

**FR35:** The system SHALL provide command-line startup with configuration file path as parameter

### Non-Functional Requirements

**NFR1:** The system SHALL run on macOS 12+ (Monterey or newer) with Apple Silicon (M1/M2/M3) processors

**NFR2:** CoreML object detection inference SHALL complete in <100ms per frame on M1 Neural Engine

**NFR3:** LLM vision inference SHALL complete in <5 seconds per event on M1 hardware

**NFR4:** The system SHALL maintain <30% average CPU usage during continuous operation on M1 MacBook Pro

**NFR5:** The system SHALL consume <8GB RAM during normal operation with single camera stream

**NFR6:** The system SHALL detect when memory usage exceeds 8GB threshold and perform graceful shutdown with clear error message (hard stop)

**NFR7:** The system SHALL operate continuously for 24+ hours without memory leaks or crashes under normal conditions

**NFR8:** Motion detection SHALL filter out >90% of static frames to reduce LLM processing load

**NFR9:** The system SHALL process >95% of motion-triggered frames without queue overflow or dropped events

**NFR10:** Total storage for 30 days of event data (logs + annotated images) SHALL remain <4GB

**NFR11:** The system SHALL maintain M1 processor temperature <70Â°C during sustained operation (no thermal throttling)

**NFR12:** All dependencies SHALL be open source with zero licensing costs

**NFR13:** The system SHALL operate with 100% local processing and zero external API calls or cloud dependencies

**NFR14:** Network traffic SHALL be limited to RTSP stream from local camera (no internet connectivity required)

**NFR15:** Documentation SHALL enable a developer to run the system in <30 minutes from git clone

**NFR16:** README SHALL include the following sections with minimum content: Setup (installation steps), Configuration (parameter reference table), Architecture (component diagram + data flow), Troubleshooting (FAQ with â‰¥5 common issues)

**NFR17:** All code SHALL be Python 3.10+ for M1 compatibility and ML ecosystem support

**NFR18:** The system SHALL start and display health check summary within 10 seconds of launch (excluding RTSP connection time)

**NFR19:** Configuration validation in dry-run mode SHALL complete within 5 seconds

**NFR20:** Hot-reload of configuration (SIGHUP) SHALL apply changes within 2 seconds without dropping in-flight frames

**NFR21:** Graceful shutdown SHALL complete within 5 seconds, ensuring all logs are flushed and in-flight events are saved

**NFR22:** Logging and metrics collection overhead SHALL consume <5% of total CPU usage to avoid impacting processing performance

**NFR23:** The system SHALL ensure data integrity for all event logs and annotated images during graceful shutdown (no corrupted files)

**NFR24:** Minimum dependency versions SHALL be: OpenCV 4.8+, Python 3.10+, coremltools 7.0+, Ollama Python client 0.1.0+

**NFR25:** Platform-specific code (M1/CoreML/macOS APIs) SHALL be isolated in dedicated modules (e.g., platform/ directory) with abstract interfaces, enabling â‰¥80% of core logic to be platform-independent

**NFR26:** All signal handling (SIGINT, SIGTERM, SIGHUP) SHALL be thread-safe to prevent race conditions during shutdown or reload, verified by stress test sending 100 rapid signals with zero crashes or data corruption

**NFR27:** Frame processing queue SHALL have a maximum size of 100 frames to prevent unbounded memory growth during processing spikes

**NFR28:** Code documentation coverage SHALL be â‰¥80% for all public functions, classes, and modules

**NFR29:** The system SHALL include unit tests achieving â‰¥70% code coverage for core processing logic (motion detection, event de-duplication, configuration validation)

---

### NFR Verification & Testing

All 29 Non-Functional Requirements defined above are fully measurable and testable with clear pass/fail criteria. Comprehensive verification strategy, test cases, automation approach, and execution schedule are documented in:

**ðŸ“„ [Test Plan: docs/test-plan.md](test-plan.md)**

**Test Plan Summary:**
- **27 Detailed Test Cases** covering all 29 NFRs
- **100% Automated** verification using pytest, psutil, tcpdump, and coverage tools
- **CI/CD Integration** with GitHub Actions for continuous validation
- **Test Schedule:** Per-commit (platform/security), per-PR (quality/performance), nightly (24hr longevity), weekly (chaos/stress)
- **Success Criteria:** All 29 NFRs must pass before release

**Key Test Categories:**
| Category | NFRs | Test Cases | Automation |
|----------|------|------------|------------|
| Performance | NFR2-4, 8-9, 11, 18-20, 22 | TC-PERF-001 to TC-PERF-010 | pytest benchmarks |
| Resources | NFR5-6, 10, 27 | TC-RESOURCE-001 to TC-RESOURCE-004 | Monitoring daemons |
| Reliability | NFR7, 21, 23 | TC-RELIABILITY-001 to TC-RELIABILITY-003 | Longevity tests |
| Platform | NFR1, 17, 24-25 | TC-PLATFORM-001 to TC-PLATFORM-004 | Version checks |
| Security | NFR12-14 | TC-SECURITY-001 to TC-SECURITY-003 | Network analysis |
| Quality | NFR15-16, 28-29 | TC-QUALITY-001 to TC-QUALITY-004 | Coverage tools |
| Concurrency | NFR26 | TC-CONCURRENCY-001 | Stress testing |

**Critical NFRs (Must Pass for Release):**
- NFR2, NFR3: Inference speed targets (CoreML <100ms, LLM <5s)
- NFR4, NFR5: Resource limits (CPU <30%, RAM <8GB)
- NFR7: 24hr uptime without crashes
- NFR13: 100% local processing verification
- NFR29: Test coverage â‰¥70%

See [test-plan.md](test-plan.md) for complete test case details, automation scripts, and CI/CD configuration.

---

## User Interface Design Goals

### MVP Scope: CLI Interface

The MVP delivers a **command-line interface only** with no graphical UI. All user interaction occurs through:
- Terminal output (health checks, runtime metrics, event logs)
- Configuration files (YAML editing)
- Command-line flags (--help, --dry-run, --version)
- Signal handling (SIGINT, SIGHUP)

**CLI UX Principles:**
- **Clarity:** All output is human-readable with clear labels and formatting
- **Responsiveness:** Immediate feedback for user actions (startup health check, dry-run validation)
- **Transparency:** Verbose logging options for debugging and learning
- **Professional:** Standard CLI patterns (--help, exit codes, signal handling)

### Phase 2 Vision: Web Dashboard

While not in MVP scope, the following UI/UX vision guides architectural decisions to ensure Phase 2 web dashboard can be built without core refactoring:

#### Overall UX Vision

**Primary User Goal:** Monitor video recognition events in real-time with minimal friction, quickly understand what happened and when, and drill into details on demand.

**Design Philosophy:**
- **Dashboard-First:** Single-page application optimized for 4K display on large monitor
- **Real-Time Updates:** Live event feed with WebSocket/SSE for instant notifications
- **Performance-Aware:** Display system health (CPU, memory, inference times) alongside events to support learning goal
- **Privacy-Conscious:** Clean, minimal design with no external CDNs or cloud dependencies (all assets local)

**Key User Flows:**
1. **Monitoring:** Glance at dashboard to see recent events and system health
2. **Investigation:** Click event to see full details (annotated image, JSON metadata, LLM description)
3. **Search:** Filter events by time range, object type, camera (Phase 2 multi-camera)
4. **Configuration:** Adjust thresholds and settings (advanced: hot-reload via UI)

#### Key Interaction Paradigms

**Real-Time Event Stream:**
- Auto-scrolling event feed (newest at top)
- Event cards with: timestamp, thumbnail (annotated), semantic description, detected objects
- Click to expand full details
- Pause/resume auto-scroll for investigation

**System Health Monitor:**
- Live metrics dashboard (CPU, memory, inference times, event rate, frame processing time, end-to-end latency)
- System availability indicator (uptime percentage, last 24hr)
- Historical graphs (last 1hr, 24hr, 7 days)
- Status indicators (green/yellow/red) for key thresholds

**Event Detail View:**
- Full-resolution annotated image with bounding boxes
- Complete JSON metadata (formatted, collapsible sections)
- LLM semantic description (highlighted)
- Timeline context (events before/after)
- **Multi-Camera Correlation (Phase 2):** Show related events from other cameras within same time window (e.g., "person detected at front door" correlated with "vehicle detected in driveway")

**Search & Filter:**
- Quick filters: Last hour, Last 24hr, Last 7 days
- Advanced filters: Object type, confidence threshold, camera ID (Phase 2)
- Free-text search of semantic descriptions

#### Core Screens and Views

**1. Main Dashboard** (Primary View)
- **Top:** System health bar (CPU, memory, status, uptime)
- **Left Sidebar:** Filters and search
- **Center:** Real-time event feed (scrollable cards)
- **Right Panel:** Selected event details (or collapsed when nothing selected)

**2. Event Detail Modal**
- Full-screen overlay when clicking event
- Large annotated image display
- Tabbed interface: Image | Metadata | Timeline
- Close to return to dashboard

**3. System Metrics Page**
- Performance graphs over time
- Inference speed trends (CoreML, LLM)
- Frame processing time distribution
- End-to-end latency (motion detection â†’ event logged)
- System availability (uptime, downtime events)
- Resource usage history (CPU, memory, disk)
- Event frequency analysis

**4. Settings Page**
- **Web-based YAML Editor:** Editable text area with syntax highlighting for direct YAML editing
- **Form-Based Editor:** Alternative UI with form fields for common parameters (motion threshold, frame sampling rate, logging level)
- **Validation:** Real-time syntax checking and schema validation as user types
- **Apply Changes:** Hot-reload button to apply configuration without restarting system (via SIGHUP)
- **Validation Feedback:** Clear error messages for invalid YAML or out-of-range values
- **Diff View:** Show changes from current configuration before applying
- **Rollback:** Revert to previous configuration if issues occur

**5. Logs Page** (Future)
- Searchable system logs
- Filterable by log level (DEBUG, INFO, WARNING, ERROR)
- Downloadable log exports

#### Accessibility

**Target Level:** WCAG AA compliance (Phase 2)

**Authentication:** None required for MVP and Phase 2 - system is local-only on trusted network, accessed via localhost or local IP. Future authentication (Phase 3+) could add HTTP Basic Auth or OIDC if needed.

**Key Considerations:**
- Keyboard navigation throughout (tab order, shortcuts)
- Screen reader support for event descriptions
- High contrast mode option
- Resizable text (respect browser zoom)
- No color-only information (use icons + text labels)

#### Branding

**Visual Style:** Minimal, technical, developer-focused

**Aesthetic Inspiration:**
- Terminal/CLI aesthetic (monospace fonts for code/JSON)
- Dark mode by default (optional light mode)
- Syntax highlighting for JSON metadata
- Clean, spacious layout (avoid clutter)

**Color Palette:**
- Background: Dark gray (#1e1e1e) / Light white (#ffffff)
- Primary accent: Blue (#007acc) for interactive elements
- Success: Green (#4caf50) for healthy status
- Warning: Orange (#ff9800) for thresholds approaching
- Error: Red (#f44336) for failures
- Text: Light gray (#d4d4d4) / Dark gray (#333333)

**Typography:**
- Headings: Sans-serif (system font stack: -apple-system, BlinkMacSystemFont, "Segoe UI")
- Body: Sans-serif
- Code/JSON: Monospace (Menlo, Monaco, "Courier New")

#### Target Device and Platforms

**Primary:** Web Responsive - Desktop-first, optimized for 4K monitors (3840Ã—2160)

**Supported Browsers:**
- Chrome 100+ (primary development target)
- Safari 15+ (macOS users)
- Firefox 100+ (privacy-conscious users)

**Device Support:**
- Desktop: 27"+ 4K monitor (primary use case)
- Laptop: 13"-16" retina displays (MacBook Pro)
- Tablet: iPad Pro (responsive fallback, limited functionality acceptable)
- Mobile: Not prioritized (monitoring system, not mobile-first)

---

## Technical Assumptions

The following technical decisions guide the Architecture and constrain implementation choices. These assumptions are based on the project's goals (learning, privacy, M1 optimization) and constraints (zero budget, solo developer, local-only).

### Repository Structure

**Decision: Monorepo**

**Rationale:**
- **Simplicity:** Single repository simplifies solo developer workflow (one clone, one dependency install, one deploy)
- **Code Sharing:** Core logic, utilities, and types shared between CLI and future web dashboard without package management overhead
- **Atomic Changes:** Changes spanning processing engine + API can be committed together
- **Learning Goal:** Easier to understand entire system in one codebase for portfolio/learning purposes

**Structure:**
```
video-recognition/
â”œâ”€â”€ core/               # Core processing logic (platform-independent)
â”‚   â”œâ”€â”€ motion.py       # Motion detection
â”‚   â”œâ”€â”€ events.py       # Event de-duplication, logging
â”‚   â”œâ”€â”€ config.py       # Configuration validation
â”‚   â””â”€â”€ pipeline.py     # Processing pipeline orchestration
â”œâ”€â”€ platform/           # Platform-specific code (M1/macOS)
â”‚   â”œâ”€â”€ coreml_detector.py    # CoreML object detection
â”‚   â””â”€â”€ thermal.py            # M1 thermal monitoring
â”œâ”€â”€ integrations/       # External service integrations
â”‚   â”œâ”€â”€ rtsp.py         # RTSP camera client
â”‚   â””â”€â”€ ollama.py       # Ollama LLM client
â”œâ”€â”€ api/                # FastAPI server (Phase 2)
â”‚   â”œâ”€â”€ server.py       # API routes
â”‚   â””â”€â”€ websocket.py    # Real-time event streaming
â”œâ”€â”€ web/                # Web dashboard frontend (Phase 2)
â”‚   â”œâ”€â”€ index.html      # SPA entry point
â”‚   â””â”€â”€ app.js          # Frontend logic
â”œâ”€â”€ config/             # Configuration schemas and examples
â”‚   â”œâ”€â”€ config.example.yaml
â”‚   â””â”€â”€ schema.json     # JSON schema for validation
â”œâ”€â”€ tests/              # Test suite
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ performance/
â”œâ”€â”€ docs/               # Documentation
â”œâ”€â”€ main.py             # CLI entry point
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md
```

**Future-Proofing:** Clear separation (core/, platform/, api/, web/) enables future multi-platform support without refactoring

---

### Service Architecture

**Decision: Monolith (MVP) â†’ Modular Monolith (Phase 2) â†’ Future Microservices (Phase 3+)**

**MVP Architecture:**
- **Single Python Process:** All components run in one process
- **Threaded RTSP Capture:** Dedicated thread for continuous frame capture
- **Sequential Processing:** Motion detection â†’ CoreML â†’ LLM in main thread (simplicity over parallelism)
- **Shared Memory:** Frame queue in-memory (max 100 frames per NFR27)

**Rationale for MVP:**
- Solo developer maintainability
- Simpler debugging (single process, no IPC)
- Sufficient performance for single camera (motion-triggered processing reduces load)
- Faster MVP delivery

**Phase 2 Architecture (Modular Monolith):**
- **Core Processing Service:** Runs as before (single process)
- **FastAPI Server:** Separate process communicating via filesystem (reads event logs/images) or IPC
- **WebSocket Server:** Real-time event streaming to web dashboard
- **Shared Data:** SQLite database for event queries, filesystem for images

**Rationale for Phase 2:**
- Web dashboard doesn't block processing engine
- API server can restart independently for updates
- Still simple deployment (two processes, no orchestration)

**Phase 3+ Considerations (Out of Scope):**
- Message queue (RabbitMQ, Redis) for multi-camera scaling
- Separate services: Camera Manager, Object Detector, LLM Processor, Event Aggregator
- Only if performance requires it (6+ cameras, complex workflows)

---

### Programming Languages & Frameworks

#### Core Processing

**Language: Python 3.10+**

**Rationale:**
- **M1 Native Support:** Python 3.10+ runs natively on Apple Silicon (no Rosetta translation)
- **ML Ecosystem:** Best-in-class libraries for computer vision (OpenCV), ML (coremltools), and LLM integration (Ollama client)
- **Rapid Development:** Solo developer can iterate quickly with Python's expressiveness
- **Learning Goal:** Python is accessible for portfolio review and knowledge sharing
- **Performance:** Acceptable when heavy lifting delegated to CoreML (C++/Metal) and Ollama (Go/C++)

**Alternatives Considered:**
- **Go:** Better concurrency, faster, but lacks ML ecosystem and CoreML bindings
- **Rust:** Maximum performance, but steep learning curve and limited ML libraries
- **C++:** Direct Metal access, but development time 3-5x longer for solo dev

**Key Libraries:**
- **OpenCV 4.8+:** Motion detection, frame manipulation, RTSP client
- **coremltools 7.0+:** CoreML model loading and inference, ANE compatibility
- **ollama-python 0.1.0+:** Ollama API client for LLM inference
- **PyYAML:** Configuration file parsing
- **Pydantic:** Configuration validation and schema enforcement
- **psutil:** System resource monitoring (CPU, memory)

#### API Server (Phase 2)

**Framework: FastAPI**

**Rationale:**
- **Async Support:** Handles WebSocket connections efficiently for real-time event streaming
- **Auto-Generated Docs:** OpenAPI/Swagger UI for API exploration (supports learning goal)
- **Type Safety:** Leverages Python type hints with Pydantic for validation
- **Performance:** Comparable to Node.js/Go for I/O-bound operations (API requests)
- **Developer Experience:** Simple routing, dependency injection, built-in testing support

**Alternatives Considered:**
- **Flask:** Simpler but lacks async and auto-docs
- **Django:** Too heavy, includes ORM/admin we don't need
- **Node.js (Express):** Strong ecosystem but adds second language to project

#### Web Dashboard Frontend (Phase 2)

**Approach: Vanilla JavaScript (HTML/CSS/JS) â†’ Future Framework if needed**

**Rationale for Vanilla JS:**
- **Zero Build Step:** Simple development, no webpack/vite/parcel complexity
- **Learning Goal:** Understanding fundamentals before frameworks
- **Minimal Dependencies:** No npm package management, no security vulnerabilities in frontend deps
- **Performance:** Native browser APIs are fast, no framework overhead
- **Solo Dev:** Avoid framework churn (React 18 â†’ 19, Vue 3 â†’ 4, etc.)

**Technology Stack:**
- **HTML5:** Semantic markup
- **CSS3:** Modern layout (Grid, Flexbox), CSS variables for theming
- **JavaScript ES6+:** Modules, async/await, fetch API
- **WebSocket API:** Real-time event streaming from FastAPI
- **Local Storage API:** User preferences (dark/light mode, filter settings)

**Future Framework Consideration:**
- If complexity grows beyond ~500 lines of JS, consider **Vue.js 3** or **React 18**
- Trigger: Need for component reusability, complex state management, or routing

**Styling:**
- **No CSS Framework:** Custom CSS for full control and minimal bloat
- **Alternative if needed:** TailwindCSS (utility-first, no unused CSS if purged)

---

### Data Storage

#### Event Data

**Database: SQLite**

**Rationale:**
- **Zero Config:** File-based, no server process to manage
- **Local-First:** Aligns with privacy goal, all data stays on disk
- **Sufficient Performance:** Handles 1000s of events easily (50/day Ã— 365 days = 18,250 events/year)
- **ACID Compliance:** Reliable for event logging
- **SQL Query Support:** Enables Phase 2 web dashboard search/filter
- **Portability:** Single .db file, easy backup

**Schema (Initial):**
```sql
CREATE TABLE events (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp DATETIME NOT NULL,
    camera_id TEXT NOT NULL,
    motion_confidence REAL,
    detected_objects TEXT,  -- JSON array
    llm_description TEXT,
    image_path TEXT,
    json_log_path TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_events_timestamp ON events(timestamp);
CREATE INDEX idx_events_camera ON events(camera_id);
```

**Alternatives Considered:**
- **PostgreSQL:** Overkill for single-user local app, requires server process
- **JSON Files Only:** Simple but no query capability for Phase 2 dashboard
- **NoSQL (MongoDB):** Adds complexity, no advantage for structured event data

#### Configuration

**Format: YAML Files**

**Rationale:**
- **Human-Readable:** Easy to edit manually, supports comments
- **Git-Friendly:** Text format works well with version control
- **Python Native:** PyYAML is mature and well-supported
- **Hierarchical:** Natural structure for nested config (camera settings, processing params, etc.)

**File: config.yaml**
```yaml
# Camera Configuration
camera:
  rtsp_url: "rtsp://192.168.1.100:554/stream"
  username: "admin"
  password: "password"

# Processing Configuration
processing:
  motion_threshold: 25
  frame_sampling_rate: 10
  object_blacklist: ["cat", "tree"]

# Model Configuration
models:
  coreml_model: "models/yolov3.mlmodel"
  ollama_model: "llava:latest"

# Storage Configuration
storage:
  event_data_dir: "data/events"
  max_storage_gb: 4

# Logging Configuration
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  console: true
  file: "logs/app.log"
```

**Validation:** Pydantic models validate config on load with detailed error messages

**Alternatives Considered:**
- **JSON:** No comments, less human-readable
- **TOML:** Less familiar to users, similar benefits to YAML
- **Python Files:** Executable config is security risk

#### Logs

**Format: JSON (structured) + Plaintext (human-readable)**

**Rationale:**
- **Dual Format:** JSON for programmatic access, plaintext for manual review
- **No Database Required:** Filesystem is sufficient for logs
- **Rotation:** Simple FIFO deletion when storage limit reached (NFR10)

**Example JSON Log:**
```json
{
  "timestamp": "2025-11-08T14:32:15Z",
  "event_id": "evt_12345",
  "camera": "front_door",
  "detected_objects": [
    {"label": "person", "confidence": 0.92, "bbox": [100, 150, 200, 400]},
    {"label": "package", "confidence": 0.87, "bbox": [180, 380, 250, 450]}
  ],
  "llm_description": "Person in blue shirt carrying brown package approaching front door",
  "image_path": "data/events/2025-11-08/evt_12345.jpg"
}
```

**Example Plaintext Log:**
```
[2025-11-08 14:32:15] EVENT: Person detected at front door (confidence: 92%)
  - Objects: person (92%), package (87%)
  - Description: Person in blue shirt carrying brown package approaching front door
  - Image: data/events/2025-11-08/evt_12345.jpg
```

---

### Testing Requirements

**Strategy: Comprehensive Testing Pyramid**

**Unit Testing:**
- **Coverage Target:** â‰¥70% for core logic (NFR29)
- **Framework:** pytest
- **Scope:** Core processing functions (motion detection, event de-duplication, config validation)
- **Mocking:** Mock external services (Ollama, RTSP) for isolated tests

**Integration Testing:**
- **Scope:** End-to-end pipeline with real dependencies
- **Test Data:** Recorded RTSP streams, known event sequences
- **Validation:** Verify event logs match expected outputs

**Performance Testing:**
- **Scope:** All NFRs (see test-plan.md)
- **Tools:** pytest-benchmark, psutil for profiling
- **Critical Tests:** CoreML inference <100ms, LLM <5s, 24hr uptime

**Manual Testing:**
- **Setup Validation:** Fresh install in <30min (NFR15)
- **UI Testing (Phase 2):** Web dashboard functionality, browser compatibility

**Test Environment:**
- **CI/CD:** GitHub Actions with macOS M1 runners
- **Pre-commit Hooks:** Platform checks, license audit, coverage threshold
- **Nightly Tests:** 24hr longevity, resource monitoring

---

### Deployment & Infrastructure

**Development Environment:**
- **Platform:** macOS 13+ on M1 MacBook Pro
- **Python:** 3.10+ with virtual environment (venv)
- **Ollama:** Local installation with LLaVA/Moondream models

**Production Deployment (24/7 Operation):**
- **Platform:** Mac Mini or Mac Studio (M1/M2)
- **Startup:** systemd-equivalent (launchd on macOS) for auto-start
- **Monitoring:** Built-in metrics logging, no external monitoring needed for MVP

**Containerization:**
- **MVP:** Not required (single binary + dependencies)
- **Phase 3+:** Docker for Linux deployment (requires platform abstraction)

**Version Control:**
- **Git:** GitHub repository (public for open source)
- **Branching:** main (stable) + feature branches
- **Releases:** Semantic versioning (v1.0.0, v1.1.0, etc.)

---

### Security & Privacy

**Network Isolation:**
- **No Internet Required:** System operates on local network only
- **Firewall:** Edge firewall blocks external access (NFR14)
- **RTSP Only:** Network traffic limited to camera stream (NFR13)

**Data Privacy:**
- **100% Local:** All processing on-premises (NFR13)
- **No Cloud APIs:** Zero external service calls
- **Data Retention:** User-controlled via storage limits

**Authentication:**
- **MVP/Phase 2:** None (local-only access via localhost)
- **Phase 3+:** HTTP Basic Auth or OIDC if remote access needed

**Dependency Security:**
- **License Compliance:** All dependencies open source (NFR12)
- **Vulnerability Scanning:** `pip-audit` in CI/CD to detect CVEs
- **Pinned Versions:** requirements.txt pins exact versions for reproducibility

---

### Performance Optimization

**Apple Silicon Optimization:**
- **CoreML on ANE:** Object detection runs on Apple Neural Engine (NFR2: <100ms)
- **Metal Acceleration:** GPU acceleration for image processing where available
- **Native ARM64:** All dependencies compiled for M1 (no Rosetta translation)

**Processing Efficiency:**
- **Motion-Triggered:** Process <1% of frames via motion detection filtering (NFR8: >90% filtered)
- **Frame Sampling:** Configurable skip rate (every Nth frame) to reduce load
- **Queue Management:** Max 100 frame queue to prevent memory growth (NFR27)

**Resource Management:**
- **Memory Monitoring:** Auto-shutdown at >8GB (NFR6)
- **Thermal Awareness:** Monitor temperature, reduce processing if >65Â°C to prevent throttling
- **Disk Management:** Simple FIFO rotation when storage limit reached

---

### Additional Technical Assumptions

**Error Handling:**
- **Fail Fast:** Invalid config exits immediately with clear error message
- **Graceful Degradation:** Single camera failure doesn't crash entire system (Phase 2 multi-camera)
- **Recovery:** Auto-retry for transient failures (RTSP disconnects), manual intervention for persistent failures

**Logging Strategy:**
- **Structured Logging:** JSON for machine parsing, plaintext for human reading
- **Log Levels:** DEBUG (development), INFO (production), WARNING/ERROR (issues)
- **Performance:** Async logging to avoid blocking processing (NFR22: <5% CPU overhead)

**Code Organization:**
- **Type Hints:** All functions use Python type hints for IDE support and validation
- **Docstrings:** Google-style docstrings for all public functions (NFR28: â‰¥80% coverage)
- **Linting:** Black (formatting), Pylint (linting), mypy (type checking) in pre-commit hooks

**Observability:**
- **Metrics:** Collected in-memory, logged periodically (NFR26: CPU, memory, inference times, frame rate)
- **Tracing:** Simple timestamp-based tracing for performance debugging
- **Profiling:** py-spy for production profiling when needed

**Development Tools:**
- **IDE:** VS Code with Python extension (recommended for solo dev)
- **Debugging:** Built-in Python debugger (pdb) + VS Code debugger
- **Testing:** pytest with coverage reporting
- **Documentation:** Markdown (README, architecture docs), auto-generated API docs (FastAPI)

---

## Epic List

The MVP is organized into **4 sequential epics**, each delivering a significant, deployable increment of functionality. Each epic builds upon the previous, enabling progressive delivery of value while maintaining focus on the core processing pipeline.

### Epic 1: Foundation & Motion-Triggered Frame Processing

**Goal:** Establish project infrastructure and implement motion detection pipeline that connects to RTSP camera, identifies frames with motion, and filters out static scenes.

**Value Delivered:** System can connect to camera, continuously monitor for motion, and identify when something interesting happens (>10x reduction in frames requiring further processing). This validates the core performance optimization strategy and proves the M1 can handle real-time RTSP processing.

**Key Capabilities:**
- Project setup (Git, Python environment, dependencies)
- YAML configuration loading and validation
- RTSP camera connection and frame capture
- Motion detection using OpenCV background subtraction
- Frame sampling (every Nth frame)
- Basic logging framework (console output)
- Health check / system status display

**Deployment Readiness:** Can run continuously, connect to camera, and log motion events to console.

---

### Epic 2: Object Detection & Semantic Understanding

**Goal:** Integrate CoreML object detection and Ollama LLM to identify what objects are present in motion-triggered frames and generate semantic event descriptions.

**Value Delivered:** System understands what's happening (not just that something moved). Provides natural language descriptions like "person carrying package approaching door" instead of raw motion signals. This is the core differentiator from traditional motion detection systems.

**Key Capabilities:**
- CoreML model loading and Apple Neural Engine validation
- Object detection inference on motion-triggered frames
- Object blacklist filtering (ignore cats, trees, etc.)
- Annotated image generation (bounding boxes + labels)
- Ollama service integration
- Vision LLM inference (LLaVA or Moondream)
- Structured JSON output with semantic descriptions
- Event de-duplication logic (time-based suppression)

**Deployment Readiness:** System processes events end-to-end and generates annotated images + semantic descriptions (outputs to files).

---

### Epic 3: Event Persistence & Data Management

**Goal:** Implement SQLite database for event storage, dual-format logging (JSON + plaintext), and storage management to enable event querying and long-term data retention within disk constraints.

**Value Delivered:** Events are permanently recorded and queryable. Users can review historical events, search by time/object type, and analyze patterns. Provides foundation for Phase 2 web dashboard.

**Key Capabilities:**
- SQLite database schema and initialization
- Event data model and storage
- JSON log file generation (structured)
- Plaintext log file generation (human-readable)
- Storage monitoring and limits enforcement
- Log rotation logic (FIFO when storage limit reached)
- Performance metrics logging (CPU, memory, inference times)

**Deployment Readiness:** All events persist to database and log files, queryable via SQL, storage stays within 4GB limit.

---

### Epic 4: CLI Interface & Production Readiness

**Goal:** Deliver production-ready CLI tool with professional command-line interface, graceful error handling, signal management, and operational features for 24/7 deployment.

**Value Delivered:** System is robust, user-friendly, and ready for continuous operation. Developers can quickly set up, configure, validate, and monitor the system. Supports the learning goal with comprehensive help, validation feedback, and runtime metrics.

**Key Capabilities:**
- Command-line argument parsing (--help, --dry-run, --version, config file)
- Startup health check summary (config, models, Ollama, RTSP)
- Version and build information display
- Dry-run mode (validate without processing)
- Signal handling (SIGINT, SIGTERM for shutdown, SIGHUP for hot-reload)
- Graceful shutdown with log flushing and resource cleanup
- Runtime performance metrics display (configurable intervals)
- Comprehensive error handling and validation feedback
- Platform compatibility checks (macOS, M1, Python version)
- Dependency version validation
- Example configuration file (config.example.yaml)
- README with setup instructions and troubleshooting

**Deployment Readiness:** Complete production system ready for 24/7 operation on Mac Mini/Studio with full CLI, error handling, and monitoring.

---

### Epic Sequencing Rationale

**Why 4 Epics:**
- **Epic 1:** Foundation is essential - can't build anything without infrastructure and basic connectivity. Motion detection validates performance approach early.
- **Epic 2:** Intelligence layer (CoreML + LLM) is the core value proposition - must come before persistence to ensure we're saving the right data.
- **Epic 3:** Persistence enables historical analysis and prepares for web dashboard - logical after we know what we're storing.
- **Epic 4:** Production features polish the MVP into a deployable system - best done last when core functionality is stable.

**Value at Each Stage:**
- After Epic 1: Can monitor camera for motion (basic security/awareness)
- After Epic 2: Can understand what's happening with semantic descriptions (full detection capability)
- After Epic 3: Can review historical events and analyze patterns (long-term value)
- After Epic 4: Production-ready system ready for continuous deployment (complete MVP)

**Alternative Considered:**
- Could merge Epic 3 + 4 into single "Persistence & Production" epic, reducing to 3 epics total
- Decided against: Epic 3 (data) and Epic 4 (CLI/ops) are distinct concerns, each substantial enough to warrant separate epic
- Keeps individual epics focused and stories manageable for AI agent execution

---

## Epic 1: Foundation & Motion-Triggered Frame Processing

**Epic Goal:** Establish project infrastructure and implement motion detection pipeline that connects to RTSP camera, identifies frames with motion, and filters out static scenes, delivering a deployable system that validates the core performance optimization strategy.

### Story 1.1: Project Setup and Configuration Foundation

**As a** developer,
**I want** a properly initialized Python project with dependency management and YAML configuration,
**so that** I have a solid foundation to build the video recognition system with reproducible environments.

**Acceptance Criteria:**

1. Git repository initialized with .gitignore for Python (excludes __pycache__, *.pyc, venv/, .env, data/, logs/)
2. Python virtual environment setup documented in README with commands for creation and activation
3. requirements.txt file created with pinned versions: opencv-python>=4.8.0, pyyaml>=6.0, pydantic>=2.0
4. Directory structure created matching Technical Assumptions (core/, platform/, integrations/, config/, tests/, docs/)
5. config.example.yaml file created with commented examples for all required parameters (camera URL, motion threshold, frame sampling rate, logging settings)
6. Pydantic configuration model defined (ConfigModel class) with validation for all config fields
7. Configuration loading function (load_config) validates YAML against schema and returns typed ConfigModel instance
8. Invalid configuration triggers clear error message showing field name, expected type, and provided value
9. Unit tests for configuration validation covering valid config, missing required fields, invalid types, and out-of-range values
10. README.md created with sections: Overview, Setup Instructions, Configuration Guide (placeholder content acceptable)

---

### Story 1.2: RTSP Camera Connection and Frame Capture

**As a** developer,
**I want** to connect to an RTSP camera stream and continuously capture video frames,
**so that** I have a live video feed to process for motion detection.

**Acceptance Criteria:**

1. RTSP client module (integrations/rtsp.py) implements RTSPCamera class with connect() and capture_frame() methods
2. Camera connection uses opencv-python VideoCapture with RTSP URL from configuration
3. RTSP authentication (username/password) correctly handled via URL format: rtsp://user:pass@ip:port/stream
4. Connection failures (invalid URL, network timeout, auth failure) raise RTSPConnectionError with descriptive message
5. Successful connection logs message: "Connected to RTSP stream: [camera_id]"
6. capture_frame() returns numpy array (BGR format) representing current frame, or None if stream unavailable
7. Frame capture runs in dedicated background thread to avoid blocking main processing
8. Thread-safe frame queue (max 100 frames per NFR27) buffers frames from RTSP thread to main processing thread
9. Graceful handling of stream interruptions: logs warning, attempts reconnection with exponential backoff (1s, 2s, 4s, 8s, max 60s)
10. Unit tests with mocked VideoCapture verify connection, authentication, frame capture, error handling, and reconnection logic
11. Manual integration test documented: Connect to real RTSP camera, verify frames captured at expected rate (15 fps)

---

### Story 1.3: Motion Detection Implementation

**As a** developer,
**I want** to detect motion in video frames using OpenCV background subtraction,
**so that** I can filter out static scenes and only process frames with activity.

**Acceptance Criteria:**

1. Motion detection module (core/motion.py) implements MotionDetector class with detect_motion(frame) method
2. Uses OpenCV BackgroundSubtractorMOG2 algorithm for motion detection (handles lighting changes, shadows)
3. motion_threshold parameter from configuration controls sensitivity (0-255, lower = more sensitive)
4. detect_motion() returns tuple: (has_motion: bool, confidence: float, motion_mask: numpy array)
5. has_motion is True when percentage of changed pixels exceeds threshold (default: 2% of frame)
6. confidence score represents percentage of frame with detected motion (0.0 to 1.0)
7. motion_mask is binary image showing areas of detected motion (useful for debugging/visualization)
8. Background model learns over first 100 frames to establish baseline (ignores motion during learning phase)
9. Motion detector resets background model on manual trigger (via future API) or after major scene change
10. Performance: Motion detection completes in <50ms per frame on M1 (measured via unit test with time.perf_counter)
11. Unit tests verify: motion detected in test videos with known movement, static scenes return has_motion=False, threshold configuration works correctly
12. Edge cases tested: sudden lighting change, gradual lighting change (sunrise/sunset), shadow movement

---

### Story 1.4: Frame Sampling and Processing Pipeline

**As a** developer,
**I want** to implement configurable frame sampling to process every Nth frame,
**so that** I can optimize performance by reducing the frame processing rate while maintaining adequate coverage.

**Acceptance Criteria:**

1. Frame sampling logic implemented in core/pipeline.py with FrameSampler class
2. frame_sampling_rate parameter from configuration determines sampling (e.g., 10 = process every 10th frame)
3. FrameSampler.should_process(frame_count) returns True/False based on sampling rate
4. frame_count is continuously incrementing counter of total frames captured (not just processed)
5. Sampling applies AFTER motion detection: Sample from motion-triggered frames, not all frames
6. Processing pipeline orchestration (ProcessingPipeline class) coordinates: frame capture â†’ motion detection â†’ sampling â†’ (future: object detection)
7. Pipeline maintains metrics: total_frames_captured, frames_with_motion, frames_sampled, frames_processed
8. Metrics accessible via get_metrics() method returning dictionary
9. Pipeline runs continuously in main thread, reading from frame queue populated by RTSP thread
10. Ctrl+C (SIGINT) cleanly stops pipeline: stops RTSP thread, processes remaining queued frames, logs final metrics
11. Unit tests verify sampling logic: rate=1 processes all frames, rate=10 processes every 10th, rate=100 processes every 100th
12. Integration test: Run pipeline with test video, verify metrics match expected values based on known motion events

---

### Story 1.5: Basic Logging Framework and Console Output

**As a** developer,
**I want** structured logging with configurable verbosity levels,
**so that** I can debug issues during development and monitor the system in production.

**Acceptance Criteria:**

1. Python logging configured in core/logging_config.py with setup_logging(config) function
2. Logging level configurable via YAML: DEBUG, INFO, WARNING, ERROR
3. Console handler outputs to stdout with formatted messages: "[TIMESTAMP] [LEVEL] [MODULE] Message"
4. Timestamp format: ISO 8601 with timezone (2025-11-08T14:32:15-08:00)
5. Module name included in log output for easy debugging (e.g., [rtsp], [motion], [pipeline])
6. Structured logging metadata available for future JSON output (not yet implemented, but architecture supports it)
7. Key events logged at INFO level: RTSP connected, motion detected, frame processed, metrics summary
8. Errors logged at ERROR level with full exception stack traces
9. DEBUG level logs frame-by-frame processing details (frame number, motion confidence, sampling decision)
10. Logging performance overhead measured: <2% CPU impact at INFO level, <5% at DEBUG level (verified via profiling)
11. Unit tests verify correct log level filtering (DEBUG messages don't appear when level=INFO)
12. Manual test: Run system with different log levels, verify appropriate messages appear/disappear

---

### Story 1.6: Startup Health Check and System Status Display

**As a** developer,
**I want** a startup health check that validates all dependencies and displays system status,
**so that** I can quickly identify configuration or connectivity issues before processing begins.

**Acceptance Criteria:**

1. Health check module (core/health_check.py) implements HealthChecker class with run_checks() method
2. Startup sequence performs checks in order: config validation â†’ RTSP connection â†’ initial status display
3. Each check displays status: "âœ“ Configuration loaded: config.yaml" or "âœ— RTSP connection failed: [error details]"
4. Health check summary displayed before processing starts:
   ```
   ===== System Health Check =====
   âœ“ Configuration loaded: config.yaml
   âœ“ RTSP stream: Connected (front_door)
   âœ“ Motion detector: Initialized
   âœ“ Frame queue: Ready (0/100)
   ===== System Ready =====
   ```
5. If any check fails, system exits with non-zero exit code and clear error message explaining what failed and how to fix
6. Runtime status display every 60 seconds (configurable) showing metrics:
   ```
   [INFO] Runtime Status (60s interval)
     Frames Captured: 1,847 (30.8 fps)
     Motion Detected: 127 (6.9% hit rate)
     Frames Sampled: 13 (sampling rate: 1/10)
     Queue Size: 3/100
   ```
7. Status display uses human-readable formatting: comma separators for large numbers, percentage with 1 decimal
8. status_interval configurable in YAML (default: 60 seconds, 0 = disabled)
9. Unit tests verify health check logic: all pass scenario, single failure scenario, multiple failure scenario
10. Integration test: Start system with invalid config, verify clear error message and non-zero exit code

---

### Story 1.7: Basic CLI Entry Point and Execution

**As a** developer,
**I want** a command-line entry point that initializes the system and runs the processing pipeline,
**so that** I can start the video recognition system with a simple command.

**Acceptance Criteria:**

1. CLI entry point in main.py with main() function as execution starting point
2. Command-line argument: config file path (positional argument, required)
   ```bash
   python main.py config.yaml
   ```
3. If no config file provided, display usage message and exit:
   ```
   Usage: python main.py <config-file>
   Example: python main.py config.yaml
   ```
4. main() orchestrates full startup sequence:
   - Load configuration from specified file
   - Setup logging based on config
   - Run health checks
   - Initialize RTSP connection
   - Initialize motion detector
   - Start processing pipeline
   - Run until interrupted (Ctrl+C)
5. Graceful shutdown on SIGINT (Ctrl+C): Stop RTSP thread â†’ flush frame queue â†’ log final metrics â†’ exit with code 0
6. Uncaught exceptions logged with full stack trace and exit with code 1
7. Main execution wrapped in `if __name__ == "__main__":` for proper module import behavior
8. requirements.txt updated with all dependencies used: opencv-python, pyyaml, pydantic, numpy
9. README updated with "Quick Start" section showing how to run the system
10. Integration test: Full end-to-end execution with test RTSP stream, verify system starts, processes frames, and stops cleanly on Ctrl+C

---

### Story 1.8: Unit Testing Framework and Initial Test Coverage

**As a** developer,
**I want** a pytest-based testing framework with initial unit tests for core logic,
**so that** I can verify functionality and prevent regressions as I add features.

**Acceptance Criteria:**

1. pytest installed and added to requirements-test.txt (separate from runtime requirements)
2. Test directory structure created: tests/unit/, tests/integration/, tests/conftest.py
3. conftest.py contains shared fixtures: sample_config, mock_rtsp_camera, sample_frame
4. Unit tests for configuration validation (tests/unit/test_config.py): valid config loads, invalid config raises error, missing fields detected, type validation works
5. Unit tests for motion detection (tests/unit/test_motion.py): motion detected in test video, static scenes return False, threshold configuration works, performance <50ms per frame
6. Unit tests for frame sampling (tests/unit/test_sampling.py): sampling rates verified, metrics tracking accurate
7. Unit tests for RTSP client (tests/unit/test_rtsp.py): connection logic, error handling, reconnection backoff (using mocks)
8. Test coverage measurement configured: `pytest --cov=core --cov=integrations --cov-report=term`
9. Coverage target for Epic 1: â‰¥60% (will increase to 70% by Epic 4 per NFR29)
10. All tests pass: `pytest tests/unit -v` returns zero failures
11. README updated with "Running Tests" section showing pytest commands
12. CI/CD placeholder: .github/workflows/tests.yml created (runs on pull request, executes pytest)

---

**Epic 1 Summary:**
- **8 Stories** delivering foundation, RTSP connection, motion detection, and basic CLI
- **Vertical slices:** Each story delivers complete, testable functionality
- **Progressive complexity:** Starts with setup, builds to full pipeline
- **Test coverage:** Unit tests throughout, integration tests for key flows
- **Deployable:** After Epic 1, system can monitor camera for motion events and log to console

---

### Rationale for Story Breakdown:

**Why 8 stories:**
- Each story is focused (single responsibility)
- Sized for AI agent execution (2-4 hours estimated)
- Delivers testable functionality
- Logical build-up from foundation to working pipeline

**Story sequencing:**
- 1.1 (Setup) must come first - foundation for everything
- 1.2 (RTSP) before 1.3 (Motion) - need frames to detect motion in
- 1.3 (Motion) before 1.4 (Sampling) - sampling applies to motion-triggered frames
- 1.4 (Pipeline) integrates 1.2+1.3 - natural progression
- 1.5 (Logging) could be earlier but needed throughout, added when we have meaningful events to log
- 1.6 (Health Check) uses prior components - logical placement
- 1.7 (CLI) ties everything together - entry point comes after components exist
- 1.8 (Testing) throughout development - added as final story to formalize framework

**Cross-cutting concerns handled:**
- Testing: Integrated into each story's acceptance criteria + dedicated Story 1.8 for framework
- Logging: Story 1.5 establishes framework, used in all subsequent stories
- Configuration: Story 1.1 establishes pattern, used throughout
- Error handling: Built into each story's acceptance criteria

---

---

## Epic 2: Object Detection & Semantic Understanding

**Epic Goal:** Integrate CoreML object detection and Ollama LLM to identify what objects are present in motion-triggered frames and generate semantic event descriptions, delivering the core differentiator from traditional motion detection systems.

### Story 2.1: CoreML Model Loading and Neural Engine Validation

**As a** developer,
**I want** to load CoreML object detection models and verify they run on Apple Neural Engine,
**so that** I can perform fast, M1-optimized object detection on video frames.

**Acceptance Criteria:**

1. CoreML detector module (platform/coreml_detector.py) implements CoreMLDetector class with load_model(model_path) method
2. model_path from configuration specifies CoreML model file (e.g., "models/yolov3.mlmodel")
3. Model loading uses coremltools.models.MLModel API to load .mlmodel file
4. ANE (Apple Neural Engine) compatibility check verifies model will run on Neural Engine, not CPU/GPU
5. If model is not ANE-compatible, log WARNING: "Model will run on CPU/GPU (slower), consider using ANE-optimized model"
6. Model metadata extracted and logged: input shape, output format, model type (e.g., "YOLOv3-Tiny 416x416")
7. load_model() raises CoreMLLoadError with clear message if model file not found, corrupted, or incompatible
8. Successful model load logs: "âœ“ CoreML model loaded: [model_name] (ANE-compatible)"
9. Model warm-up: Run inference on dummy frame during load to initialize ANE and measure baseline inference time
10. Unit tests with test .mlmodel file verify: successful load, ANE check, error handling for missing/invalid models
11. Integration test: Load YOLOv3-Tiny CoreML model, verify ANE compatibility, measure warm-up inference time <100ms
12. README updated with section on obtaining CoreML models (link to Apple's model gallery or conversion instructions)

---

### Story 2.2: Object Detection Inference and Bounding Box Extraction

**As a** developer,
**I want** to run object detection inference on frames and extract detected objects with bounding boxes,
**so that** I know what objects are present and where they are located in the frame.

**Acceptance Criteria:**

1. CoreMLDetector.detect(frame) method accepts numpy array frame (BGR format from OpenCV)
2. Frame preprocessing: Convert BGR â†’ RGB, resize to model input size (e.g., 416x416), normalize pixel values
3. CoreML inference executed via model.predict() returning raw model output (bounding boxes, class probabilities, confidence scores)
4. Post-processing: Parse model output into list of detected objects, each containing:
   - label: str (e.g., "person", "car", "dog")
   - confidence: float (0.0 to 1.0)
   - bbox: tuple (x, y, width, height) in original frame coordinates
5. Confidence threshold filtering: Only return objects with confidence >= min_confidence (from config, default: 0.5)
6. Non-maximum suppression (NMS) applied to remove duplicate detections of same object
7. detect() returns DetectionResult object with fields: objects (list), inference_time (float in seconds), frame_shape
8. Performance requirement: Inference completes in <100ms on M1 Neural Engine (measured and logged)
9. If inference time exceeds 100ms, log WARNING with actual time
10. Unit tests verify: detection on test images with known objects, confidence filtering works, bbox coordinates are correct
11. Integration test: Run detection on sample video frames, verify objects detected match ground truth annotations
12. Performance test: Run 100 inferences, verify average time <100ms and 95th percentile <120ms

---

### Story 2.3: Object Blacklist Filtering

**As a** developer,
**I want** to filter out detected objects that match a configurable blacklist,
**so that** I can ignore irrelevant objects (like pets or trees) and reduce false positive events.

**Acceptance Criteria:**

1. object_blacklist configuration parameter accepts list of object labels to ignore (e.g., ["cat", "tree", "bird"])
2. Blacklist filtering applied in CoreMLDetector.detect() before returning results
3. Detected objects with labels matching blacklist (case-insensitive) are removed from results
4. Filtering logged at DEBUG level: "Filtered [N] blacklisted objects: [labels]"
5. If all detected objects are blacklisted, detect() returns empty DetectionResult with objects=[]
6. Empty detections do NOT trigger events (no further processing, no LLM call, no event logging)
7. Blacklist supports partial matching: "cat" matches "cat", "cats", but NOT "cattle" (exact word boundary matching)
8. Blacklist defaults to empty list if not specified in config (no filtering)
9. Unit tests verify: exact match filtering, case-insensitivity, partial match behavior, empty blacklist passes all objects
10. Integration test: Configure blacklist=["cat"], run detection on frame with cat+person, verify only person returned

---

### Story 2.4: Annotated Image Generation with Bounding Boxes

**As a** developer,
**I want** to generate annotated images with bounding boxes and labels for detected objects,
**so that** I can visually verify detection quality and provide useful output for users.

**Acceptance Criteria:**

1. Image annotation module (core/image_annotator.py) implements ImageAnnotator class with annotate(frame, detections) method
2. For each detected object, draw:
   - Bounding box rectangle in green (success color) with 2-pixel thickness
   - Label text with confidence: "[label] (XX%)" positioned above bounding box
   - Text background rectangle (semi-transparent black) for readability
3. OpenCV drawing functions used: cv2.rectangle(), cv2.putText(), cv2.getTextSize()
4. Font: cv2.FONT_HERSHEY_SIMPLEX with scale 0.6 for readability
5. Color coding by confidence: green (>0.8), yellow (0.5-0.8), red (<0.5)
6. annotate() returns annotated frame as numpy array (does not modify original frame)
7. If no detections, return original frame unmodified
8. Bounding boxes clipped to frame boundaries (no drawing outside frame)
9. Overlapping labels handled: Offset vertically if bboxes overlap to prevent text collision
10. Unit tests verify: single object annotation, multiple objects, no detections, edge cases (bbox at frame edge)
11. Integration test: Annotate frame with 5+ objects, visually inspect output (save to temp file for manual review)
12. Performance: Annotation completes in <20ms for typical frame with 10 objects

---

### Story 2.5: Ollama Service Integration and Model Verification

**As a** developer,
**I want** to connect to local Ollama service and verify the vision model is available,
**so that** I can use LLMs for semantic event description generation.

**Acceptance Criteria:**

1. Ollama client module (integrations/ollama.py) implements OllamaClient class with connect() and verify_model(model_name) methods
2. ollama_model configuration parameter specifies model (e.g., "llava:latest", "moondream")
3. Connection uses ollama Python client library to communicate with localhost:11434 (default Ollama port)
4. connect() verifies Ollama service is running by calling /api/tags endpoint
5. If Ollama service not reachable, raise OllamaConnectionError: "Ollama service not reachable at localhost:11434. Is Ollama running?"
6. verify_model() checks if specified vision model is downloaded/available via /api/show endpoint
7. If model not found, raise OllamaModelNotFoundError: "Vision model '[model]' not found. Run: ollama pull [model]"
8. Successful verification logs: "âœ“ Ollama service: Connected (localhost:11434)" and "âœ“ Vision model: [model] (available)"
9. List all available models on connect (logged at DEBUG level) for troubleshooting
10. Unit tests with mocked Ollama API verify: successful connection, service unreachable error, model not found error
11. Integration test: Connect to real Ollama instance, verify llava:latest model (assumes Ollama running locally)
12. README updated with "Prerequisites" section: Installing Ollama and downloading vision models

---

### Story 2.6: Vision LLM Inference and Semantic Description Generation

**As a** developer,
**I want** to send frames with detected objects to Ollama vision LLM for semantic description,
**so that** I can generate natural language event descriptions beyond basic object labels.

**Acceptance Criteria:**

1. OllamaClient.generate_description(frame, detections) method accepts frame and DetectionResult
2. Frame encoding: Convert numpy array to base64-encoded JPEG for Ollama API
3. Prompt engineering: Construct vision prompt with context:
   ```
   Describe what is happening in this image. Focus on: {detected_object_labels}.
   Provide a concise, natural description of the scene and any actions.
   ```
4. Ollama API call: POST to /api/generate with model, prompt, and base64 image
5. Response parsing: Extract generated text from Ollama JSON response
6. Timeout handling: 10-second timeout for LLM inference (configurable via llm_timeout in config)
7. If timeout exceeded, raise OllamaTimeoutError and log warning (do not crash processing pipeline)
8. Successful inference returns string description (e.g., "Person in blue shirt carrying brown package approaching front door")
9. Performance logged: LLM inference time measured and logged at INFO level
10. Target performance: <5 seconds for 95th percentile (verified in integration tests with multiple frames)
11. If inference exceeds 5s, log WARNING with actual time
12. Unit tests with mocked Ollama API verify: successful generation, timeout handling, prompt construction
13. Integration test: Generate description for 10 sample frames, verify descriptions are semantically accurate and relevant
14. Error handling: Ollama errors (model overload, OOM) logged but don't crash system

---

### Story 2.7: Structured Event JSON Output

**As a** developer,
**I want** to generate structured JSON output for each event with all metadata,
**so that** events can be programmatically processed and stored in the database (Epic 3).

**Acceptance Criteria:**

1. Event data model (core/events.py) defines Event dataclass with fields:
   - event_id: str (unique identifier, format: "evt_" + timestamp + random)
   - timestamp: datetime (ISO 8601 format with timezone)
   - camera_id: str (from config)
   - motion_confidence: float (from motion detection)
   - detected_objects: list[dict] (each with label, confidence, bbox)
   - llm_description: str (from Ollama)
   - image_path: str (relative path to annotated image file)
   - metadata: dict (inference times, frame number, etc.)
2. Event.to_json() method serializes to JSON string with proper formatting (indented, datetime ISO format)
3. JSON schema matches FR10 requirement: timestamp, detected objects with labels/confidence, action description, location/context
4. event_id generation ensures uniqueness using timestamp (milliseconds) + 4-character random suffix
5. camera_id defaults to "camera_1" for MVP (single camera), prepared for multi-camera in Phase 2
6. metadata includes: coreml_inference_time, llm_inference_time, frame_number, motion_threshold_used
7. Event.from_json() method deserializes JSON back to Event object (for testing and future database loading)
8. Pydantic validation ensures all required fields present and types correct
9. Unit tests verify: JSON serialization/deserialization, event_id uniqueness, datetime handling, schema compliance
10. Integration test: Create event from real detection + LLM output, serialize to JSON, verify structure matches specification

---

### Story 2.8: Event De-duplication Logic

**As a** developer,
**I want** to suppress duplicate events for the same continuous activity,
**so that** users don't receive multiple alerts for a person walking across multiple frames.

**Acceptance Criteria:**

1. Event de-duplication module (core/events.py) implements EventDeduplicator class with should_create_event() method
2. deduplication_window configuration parameter specifies time window in seconds (default: 30 seconds)
3. Deduplication strategy: Suppress events with same primary object (highest confidence) within time window
4. EventDeduplicator maintains cache of recent events: {object_label: last_event_timestamp}
5. should_create_event(detections) checks if primary object (highest confidence) appeared in last N seconds
6. If primary object in cache and time_since_last < deduplication_window, return False (suppress event)
7. If suppressed, log at DEBUG: "Event suppressed: [object] detected [X]s ago (within [N]s window)"
8. If not suppressed (new object or outside window), return True and update cache with current timestamp
9. Cache cleanup: Remove entries older than 2x deduplication_window to prevent unbounded memory growth
10. Edge case: Multiple objects detected - use highest confidence object as "primary" for deduplication key
11. Unit tests verify: duplicate suppression works, window expiry works, cache cleanup prevents memory leak
12. Integration test: Process video with person walking across frame for 10 seconds, verify only 1 event created (not 150+)
13. Deduplication metrics tracked: events_suppressed counter logged in runtime status

---

### Story 2.9: Processing Pipeline Integration - Object Detection + LLM

**As a** developer,
**I want** to integrate CoreML and Ollama into the processing pipeline,
**so that** motion-triggered frames flow through the complete intelligence stack end-to-end.

**Acceptance Criteria:**

1. ProcessingPipeline (core/pipeline.py) extended with new processing stages:
   - Stage 3: Object detection (CoreML)
   - Stage 4: Event de-duplication check
   - Stage 5: LLM semantic description (Ollama)
   - Stage 6: Event creation and output
2. Full pipeline flow: RTSP frame â†’ motion detection â†’ sampling â†’ **object detection â†’ de-duplication â†’ LLM â†’ event output**
3. For each sampled frame with motion:
   - Run CoreML detection
   - Check blacklist filter
   - If objects detected (non-empty), check de-duplication
   - If event should be created, run LLM inference
   - Generate Event object with all metadata
   - Save annotated image to disk (data/events/YYYY-MM-DD/evt_xxxxx.jpg)
   - Output Event JSON (logged to console for now, file output in Epic 3)
4. Error handling at each stage: Log error, skip frame, continue processing (don't crash)
5. Pipeline metrics extended: objects_detected, events_created, events_suppressed, coreml_time_avg, llm_time_avg
6. Metrics logged in runtime status display (every 60s)
7. Performance target: Process motion-triggered frame end-to-end in <6 seconds (motion + CoreML + LLM)
8. If processing exceeds 10 seconds, log WARNING and consider frame drop if queue backing up
9. Graceful degradation: If Ollama unavailable, log error and create event without LLM description (still save annotated image + detection data)
10. Unit tests verify: pipeline stages execute in order, error in one stage doesn't crash pipeline, metrics tracking accurate
11. Integration test: Full end-to-end execution with test RTSP stream for 5 minutes, verify events created, annotated images saved, metrics accurate
12. Manual test: Run system with real camera, trigger motion events (walk in front of camera), verify events created with semantic descriptions

---

**Epic 2 Summary:**
- **9 Stories** delivering CoreML object detection, Ollama LLM integration, and semantic event generation
- **Vertical slices:** Each story is complete, testable functionality building towards full intelligence capability
- **Core value delivered:** System now understands WHAT is happening, not just THAT something moved
- **Deployable:** After Epic 2, system generates annotated images + semantic event descriptions (outputs to files, database in Epic 3)

---

### Rationale for Story Breakdown:

**Why 9 stories:**
- CoreML integration (2.1-2.4): 4 stories cover model loading, inference, filtering, visualization
- Ollama integration (2.5-2.6): 2 stories for connection and LLM inference
- Event processing (2.7-2.9): 3 stories for structured data, de-duplication, full pipeline integration
- Each story focused and sized for AI agent execution (2-4 hours)

**Story sequencing:**
- 2.1-2.2 (CoreML) before 2.5-2.6 (Ollama) - detect objects before describing them
- 2.3 (Blacklist) after 2.2 - filtering builds on basic detection
- 2.4 (Annotation) parallel to 2.5 - visualization independent of LLM
- 2.7 (Event JSON) before 2.8 (De-dup) - need data model before filtering
- 2.9 (Integration) last - ties everything together after individual components work

**Cross-cutting concerns:**
- Testing: Unit tests in each story + integration tests for end-to-end flows
- Performance: Measured and validated in each story (CoreML <100ms, LLM <5s, total <6s)
- Error handling: Graceful degradation if Ollama unavailable, logging at each failure point
- Configuration: Blacklist, thresholds, model paths all configurable

---

---

## Epic 3: Event Persistence & Data Management

**Epic Goal:** Implement SQLite database for event storage, dual-format logging (JSON + plaintext), and storage management to enable event querying and long-term data retention within disk constraints, providing the foundation for Phase 2 web dashboard.

### Story 3.1: SQLite Database Setup and Schema Implementation

**As a** developer,
**I want** to initialize a SQLite database with a schema for event storage,
**so that** events can be persisted, queried, and analyzed over time.

**Acceptance Criteria:**

1. Database module (core/database.py) implements DatabaseManager class with init_database(db_path) method
2. db_path from configuration specifies SQLite file location (default: "data/events.db")
3. Database schema matches Technical Assumptions specification:
   ```sql
   CREATE TABLE IF NOT EXISTS events (
       id INTEGER PRIMARY KEY AUTOINCREMENT,
       event_id TEXT UNIQUE NOT NULL,
       timestamp DATETIME NOT NULL,
       camera_id TEXT NOT NULL,
       motion_confidence REAL,
       detected_objects TEXT,
       llm_description TEXT,
       image_path TEXT,
       json_log_path TEXT,
       created_at DATETIME DEFAULT CURRENT_TIMESTAMP
   );
   CREATE INDEX IF NOT EXISTS idx_events_timestamp ON events(timestamp);
   CREATE INDEX IF NOT EXISTS idx_events_camera ON events(camera_id);
   CREATE INDEX IF NOT EXISTS idx_events_event_id ON events(event_id);
   ```
4. detected_objects stored as JSON TEXT (serialize list[dict] to JSON string for SQLite storage)
5. Database created automatically if file doesn't exist (no manual setup required)
6. Database connection pooling: Single connection reused throughout application lifetime
7. init_database() verifies schema on startup: Creates tables if missing, validates existing schema
8. Schema version tracking and migration strategy:
   - Create schema_version table: `CREATE TABLE schema_version (version INTEGER PRIMARY KEY, applied_at DATETIME)`
   - Current schema version: 1 (MVP baseline)
   - Migration approach: Manual migration scripts in migrations/ directory (e.g., migrations/001_initial.sql, migrations/002_add_confidence.sql)
   - On startup, check schema_version table and apply any pending migrations in sequence
   - Each migration script wrapped in transaction for atomicity
   - Migration failure logs ERROR and exits (prevents running with incompatible schema)
   - Alternative: Document that schema changes require backup â†’ delete â†’ recreate for MVP (acceptable for Phase 1)
9. Transaction support: insert_event() wrapped in transaction for atomicity
10. Unit tests verify: database creation, schema validation, index creation, transaction handling, schema_version initialization
11. Integration test: Initialize database, insert 100 test events, verify all persisted correctly
12. Error handling: Database corruption or permission errors logged with clear message and graceful exit
13. Documentation: migrations/README.md documents migration process and includes backup script (sqlite3 .dump)

---

### Story 3.2: Event Persistence to SQLite Database

**As a** developer,
**I want** to persist Event objects to the SQLite database,
**so that** events are permanently stored and queryable for historical analysis.

**Acceptance Criteria:**

1. DatabaseManager.insert_event(event: Event) method persists Event object to database
2. Event fields mapped to database columns: event_id â†’ event_id, timestamp â†’ timestamp, etc.
3. detected_objects list serialized to JSON TEXT before storage
4. INSERT statement uses parameterized queries to prevent SQL injection
5. Duplicate event_id handling: UNIQUE constraint prevents duplicate inserts, log WARNING if duplicate detected
6. insert_event() returns success boolean: True if inserted, False if duplicate or error
7. Database write errors (disk full, permission denied) logged and raised as DatabaseWriteError
8. Performance: Insert completes in <10ms (measured and logged if exceeds threshold)
9. Batch insert support: insert_events(events: list[Event]) for efficient bulk operations
10. Unit tests verify: single event insert, duplicate handling, batch insert, error scenarios
11. Integration test: Insert 1000 events via pipeline, query database, verify all present with correct data
12. Foreign key support prepared for Phase 2 multi-camera (camera_id references cameras table in future)

---

### Story 3.3: Event Query and Retrieval API

**As a** developer,
**I want** to query events from the database with filtering,
**so that** I can retrieve historical events for analysis and future web dashboard integration.

**Acceptance Criteria:**

1. DatabaseManager implements query methods:
   - get_event_by_id(event_id) â†’ Event | None
   - get_events_by_timerange(start, end) â†’ list[Event]
   - get_recent_events(limit=100) â†’ list[Event]
   - count_events() â†’ int
2. Query results deserialized to Event objects (JSON TEXT â†’ list[dict] for detected_objects)
3. Timerange queries use indexed timestamp column for performance
4. get_events_by_timerange() accepts datetime objects with timezone awareness
5. Query results ordered by timestamp DESC (newest first) by default
6. Pagination support: offset and limit parameters for large result sets
7. Filter by camera_id: get_events_by_camera(camera_id, limit) for Phase 2 multi-camera
8. Performance: Simple queries (<100 results) complete in <50ms on database with 10,000 events
9. Unit tests verify: query by ID, timerange filtering, pagination, ordering, deserialization
10. Integration test: Insert 1000 events, query various timeranges, verify results match expected

---

### Story 3.4: JSON Event Log File Generation

**As a** developer,
**I want** to write event data to structured JSON log files,
**so that** events can be programmatically processed by external tools without database access.

**Acceptance Criteria:**

1. JSON logger module (core/json_logger.py) implements JSONEventLogger class with log_event(event) method
2. JSON files organized by date: data/events/YYYY-MM-DD/events.json (one file per day)
3. Each event appended as single JSON object on new line (JSON Lines format, not array)
4. Event serialization uses Event.to_json() method from Story 2.7
5. File rotation: New file created at midnight (based on event timestamp, not system time)
6. Atomic writes: Write to temp file, then rename to prevent corruption if process interrupted
7. Directory creation: Automatically create date directories (data/events/2025-11-08/) if missing
8. File permissions: Created with 0644 (user read/write, group/others read-only)
9. Concurrent writes: File locking or append-only mode to prevent corruption if multi-threaded (future-proofing)
10. Performance: Log write completes in <5ms
11. Unit tests verify: file creation, directory structure, JSON Lines format, atomic writes
12. Integration test: Log 100 events across 3 days, verify correct file structure and content

---

### Story 3.5: Plaintext Event Log File Generation

**As a** developer,
**I want** to write human-readable plaintext event logs,
**so that** I can quickly review events manually without parsing JSON or querying the database.

**Acceptance Criteria:**

1. Plaintext logger module (core/plaintext_logger.py) implements PlaintextEventLogger class
2. Plaintext files organized by date: data/events/YYYY-MM-DD/events.log (one file per day)
3. Log format matches Technical Assumptions specification:
   ```
   [2025-11-08 14:32:15] EVENT: Person detected at front door (confidence: 92%)
     - Objects: person (92%), package (87%)
     - Description: Person in blue shirt carrying brown package approaching front door
     - Image: data/events/2025-11-08/evt_12345.jpg
   ```
4. Timestamp in local timezone with format: YYYY-MM-DD HH:MM:SS
5. Event separator: Blank line between events for readability
6. Detected objects listed with confidence percentages
7. LLM description included on "Description:" line
8. Image path as relative path from project root
9. Metadata line (optional, at DEBUG level): Frame number, inference times
10. File rotation synchronized with JSON logger (same midnight boundary)
11. Performance: Write completes in <5ms
12. Unit tests verify: log format, multi-object display, timestamp formatting
13. Integration test: Generate logs for 50 events, manually inspect for readability and correctness

---

### Story 3.6: Storage Monitoring and Size Limits Enforcement

**As a** developer,
**I want** to monitor disk usage and enforce storage limits,
**so that** the system doesn't consume unlimited disk space (NFR10: <4GB for 30 days).

**Acceptance Criteria:**

1. Storage monitor module (core/storage_monitor.py) implements StorageMonitor class with check_usage() method
2. max_storage_gb configuration parameter specifies limit (default: 4GB)
3. check_usage() calculates total size of data/events directory (database + logs + images)
4. Size calculation uses os.path.getsize() recursively through all subdirectories
5. check_usage() returns StorageStats object: total_bytes, limit_bytes, percentage_used, is_over_limit
6. Storage check performed every 100 events (configurable via storage_check_interval)
7. If storage exceeds limit, log ERROR: "Storage limit exceeded: [X]GB / [Y]GB ([Z]%)"
8. Over-limit action (NFR28): Trigger graceful shutdown with exit code indicating storage full
9. Warning threshold: Log WARNING at 80% of limit ("Storage at 3.2GB / 4GB (80%), approaching limit")
10. Storage stats logged in runtime status display: "Storage: 1.2GB / 4GB (30%)"
11. Unit tests verify: size calculation, limit enforcement, warning thresholds
12. Integration test: Simulate storage growth to limit, verify warning and shutdown trigger

---

### Story 3.7: Log Rotation and Cleanup Strategy

**As a** developer,
**I want** to implement FIFO log rotation to manage disk space,
**so that** old logs are deleted when storage limit is approached.

**Acceptance Criteria:**

1. Log rotation module (core/log_rotation.py) implements LogRotator class with rotate_logs() method
2. Rotation strategy: Delete oldest date directories when storage >90% of limit
3. rotate_logs() identifies oldest directories by date (YYYY-MM-DD) in data/events/
4. Deletion order: Oldest first, continues until storage <80% of limit
5. Deletion includes: Date directory with all contents (JSON logs, plaintext logs, annotated images)
6. Rotation logged at WARNING level: "Storage limit approaching, deleting old logs: 2025-10-15 (freed [X]MB)"
7. Protection: Never delete events from current day (today's directory is protected)
8. Minimum retention: Keep at least 7 days of data regardless of storage (configurable via min_retention_days)
9. rotate_logs() called automatically when storage check detects >90% usage
10. Manual rotation support: rotate_logs(force=True) bypasses percentage check
11. Unit tests verify: oldest directory identification, deletion logic, protection rules
12. Integration test: Fill storage to 95%, trigger rotation, verify oldest logs deleted and storage reduced

---

### Story 3.8: Performance Metrics Collection and Logging

**As a** developer,
**I want** to collect and log system performance metrics,
**so that** I can monitor system health and validate NFR performance targets.

**Acceptance Criteria:**

1. Metrics collector module (core/metrics.py) implements MetricsCollector class with collect() method
2. Metrics collected (per NFR21, FR24-26):
   - frames_processed: total count
   - motion_detected: count and hit rate percentage
   - events_created: count
   - events_suppressed: count (de-duplication)
   - coreml_inference_time: average, min, max, p95
   - llm_inference_time: average, min, max, p95
   - cpu_usage: current and average (via psutil)
   - memory_usage: current in MB/GB (via psutil)
   - frame_processing_latency: end-to-end time from motion â†’ event logged
   - system_availability: uptime percentage
3. Metrics stored in-memory with rolling window (last 1000 events for averages)
4. collect() returns MetricsSnapshot object with all current values
5. Metrics logged to dedicated file: logs/metrics.json (JSON Lines format)
6. Metrics logged periodically: Every 60 seconds (configurable via metrics_interval)
7. Metrics included in runtime status display (console output every 60s)
8. Performance: Metrics collection overhead <1% CPU (verified via profiling)
9. Unit tests verify: metric calculation, rolling window, percentile calculation
10. Integration test: Run system for 10 minutes, verify metrics logged correctly and match expected values

---

### Story 3.9: Pipeline Integration - Database and File Persistence

**As a** developer,
**I want** to integrate database and file logging into the processing pipeline,
**so that** all events are persisted to all three storage mechanisms (database, JSON, plaintext).

**Acceptance Criteria:**

1. ProcessingPipeline extended with persistence stage (after event creation in Epic 2):
   - Stage 7: Persist event to SQLite database
   - Stage 8: Write JSON log file
   - Stage 9: Write plaintext log file
   - Stage 10: Check storage limits
2. For each created event:
   - Insert into database via DatabaseManager.insert_event()
   - Append to JSON log via JSONEventLogger.log_event()
   - Append to plaintext log via PlaintextEventLogger.log_event()
   - Every 100 events, check storage via StorageMonitor.check_usage()
3. Error handling: Database write failure doesn't prevent file logging (graceful degradation)
4. Transaction semantics: All three writes attempted even if one fails (best-effort persistence)
5. Failed writes logged at ERROR level with specific failure reason
6. Pipeline metrics extended: events_persisted, db_write_errors, file_write_errors, storage_warnings
7. Startup initialization: DatabaseManager.init_database() called during health check
8. Shutdown cleanup: Flush all buffers, close database connection gracefully
9. Unit tests verify: all persistence mechanisms called, error handling, graceful degradation
10. Integration test: Run full pipeline for 5 minutes, verify events in database, JSON files, and plaintext files
11. Storage limit test: Run until storage limit reached, verify graceful shutdown triggered
12. Manual test: Review database content with sqlite3 CLI, verify data integrity and queryability

---

**Epic 3 Summary:**
- **9 Stories** delivering SQLite persistence, dual-format logging, storage management, and metrics collection
- **Vertical slices:** Each story delivers complete persistence layer functionality
- **Core value delivered:** Events are permanently stored, queryable, and managed within storage constraints
- **Deployable:** After Epic 3, system has complete data persistence and can be queried for historical analysis (ready for Phase 2 web dashboard)

---

### Rationale for Story Breakdown:

**Why 9 stories:**
- Database foundation (3.1-3.3): 3 stories for schema, persistence, querying
- File logging (3.4-3.5): 2 stories for JSON and plaintext formats
- Storage management (3.6-3.7): 2 stories for monitoring and rotation
- Metrics (3.8): 1 story for performance tracking
- Integration (3.9): 1 story tying everything together

**Story sequencing:**
- 3.1 (Schema) before 3.2 (Insert) - need database before writing to it
- 3.2 (Insert) before 3.3 (Query) - persistence before retrieval
- 3.4-3.5 (Logging) parallel - independent file formats
- 3.6 (Monitoring) before 3.7 (Rotation) - need to check before cleaning
- 3.8 (Metrics) parallel to storage - independent concern
- 3.9 (Integration) last - combines all persistence mechanisms

**Cross-cutting concerns:**
- Testing: Unit tests for logic, integration tests for database operations
- Performance: All write operations <10ms to avoid blocking pipeline
- Error handling: Graceful degradation if one persistence mechanism fails
- Storage: Consistent enforcement of 4GB limit across all data

---

## Epic 4: CLI Interface & Production Readiness

**Epic Goal:** Deliver production-ready CLI application with professional command-line interface, robust signal handling, graceful error management, and operational features required for 24/7 deployment on Mac Mini/Studio.

### Story 4.1: Command-Line Argument Parsing and Entry Point

**As a** developer,
**I want** a professional CLI interface with standard arguments (--help, --config, --version, --dry-run),
**so that** the system is easy to use, configure, and validate before running in production.

**Acceptance Criteria:**

1. CLI entry point implemented as `main.py` with main() function
2. Argument parsing using argparse library with following options:
   - `--config PATH` or `-c PATH`: Path to YAML configuration file (default: config/config.yaml)
   - `--dry-run`: Validate configuration and connectivity without starting processing loop
   - `--version` or `-v`: Display version and build information
   - `--help` or `-h`: Display usage instructions and examples
   - `--log-level LEVEL`: Override logging level (DEBUG, INFO, WARNING, ERROR)
   - `--metrics-interval SECONDS`: Override metrics display interval (default: 60)
3. Help text includes:
   - Usage: `python main.py [OPTIONS]`
   - Description: "Local Video Recognition System - Motion detection with CoreML object detection and Ollama LLM semantic understanding"
   - Examples: `python main.py --config cameras/front-door.yaml --dry-run`
   - Exit codes: 0=success, 1=error, 2=config invalid, 3=storage full
4. Version output format: `video-recog v1.0.0 (build 2025-11-08) - Python 3.10.12 - macOS 14.2`
5. Config file validation: If --config path doesn't exist, exit with error code 2 and message
6. Argument validation: Invalid --log-level value shows error and available options
7. Entry point callable via: `python main.py` or `python -m video_recog` (package mode)
8. Shebang support: `#!/usr/bin/env python3` allows direct execution (`./main.py`)
9. Exit code consistency: All exit paths use defined exit codes (documented in README)
10. Unit tests verify: argument parsing, default values, validation logic, help text generation
11. Integration test: Launch with each argument combination, verify expected behavior
12. Manual test: Run --help and verify output is clear and accurate

---

### Story 4.2: Startup Health Check and System Validation

**As a** developer,
**I want** comprehensive startup health checks that validate all dependencies before processing begins,
**so that** configuration errors are caught early with clear error messages (NFR23, NFR28).

**Acceptance Criteria:**

1. Health check module (core/health_check.py) implements HealthChecker class with check_all() method
2. Health checks performed in sequence during startup (before processing loop):
   - **Config validation**: YAML syntax, required fields, data types (via Pydantic schema)
   - **Platform check**: Verify macOS platform, Apple Silicon architecture (M1/M2/M3)
   - **Python version**: Verify Python >=3.10 (exit if <3.10)
   - **Dependencies**: Verify OpenCV, CoreML, Ollama client versions match requirements.txt
   - **CoreML model**: Load model file, verify Neural Engine availability
   - **Ollama service**: Check Ollama running (HTTP health check), verify model available (ollama list)
   - **RTSP connectivity**: Connect to camera URL, capture test frame, verify resolution
   - **File permissions**: Verify write access to data/, logs/, config/ directories
   - **Storage availability**: Verify <4GB storage used, warn if >80%
3. Health check results displayed in console output:
   ```
   [STARTUP] Video Recognition System v1.0.0
   [CONFIG] âœ“ Configuration loaded: config/config.yaml
   [PLATFORM] âœ“ macOS 14.2 on Apple M1 (arm64)
   [PYTHON] âœ“ Python 3.10.12
   [MODELS] âœ“ CoreML model loaded: models/yolov3.mlmodel (Neural Engine enabled)
   [OLLAMA] âœ“ Ollama service running, model available: llava:latest
   [CAMERA] âœ“ RTSP connected: rtsp://192.168.1.100:554/stream (1920x1080 @ 30fps)
   [STORAGE] âœ“ Storage: 1.2GB / 4GB (30%)
   [READY] All health checks passed. Starting processing...
   ```
4. Health check failures exit immediately with clear error messages:
   - "ERROR: CoreML model not found: models/yolov3.mlmodel - Run setup script to download models"
   - "ERROR: Ollama service not running - Start Ollama with: ollama serve"
   - "ERROR: RTSP connection failed: Authentication required - Check credentials in config.yaml"
5. Dry-run mode: Perform all health checks, display results, exit with code 0 if all pass
6. Health check timeout: Each check has 10s timeout (configurable), prevents hanging on network issues
7. Detailed logging: Each check logs result at INFO level, failures at ERROR level
8. Health check results stored in HealthCheckResult object: all_passed, failed_checks, warnings
9. Startup time: All health checks complete in <30 seconds (NFR18)
10. Unit tests verify: each individual check, failure handling, timeout behavior
11. Integration test: Test each failure scenario, verify error messages and exit codes
12. Manual test: Intentionally break each dependency, verify clear error message displayed

---

### Story 4.3: Version and Build Information Display

**As a** developer,
**I want** to display version, build date, and dependency versions,
**so that** I can identify which version is running and troubleshoot compatibility issues.

**Acceptance Criteria:**

1. Version module (core/version.py) implements get_version_info() function
2. Version information stored in version.py:
   - VERSION = "1.0.0" (semantic versioning)
   - BUILD_DATE = "2025-11-08" (ISO format, set during build)
   - GIT_COMMIT = "abc123f" (short commit hash, set during build)
3. get_version_info() returns VersionInfo object containing:
   - version: Application version (VERSION)
   - build_date: Build timestamp (BUILD_DATE)
   - git_commit: Git commit hash (GIT_COMMIT)
   - python_version: Runtime Python version (sys.version)
   - platform: OS and architecture (platform.platform())
   - opencv_version: OpenCV version (cv2.__version__)
   - coreml_version: CoreML Tools version (coremltools.__version__)
   - ollama_version: Ollama client version (ollama.__version__)
4. `--version` flag displays version information in human-readable format:
   ```
   Video Recognition System v1.0.0
   Build: 2025-11-08 (commit abc123f)
   Python: 3.10.12
   Platform: macOS-14.2-arm64
   Dependencies:
     - OpenCV: 4.8.1
     - CoreML Tools: 7.0.0
     - Ollama: 0.1.0
   ```
5. Version info logged at startup (INFO level) before health checks
6. Version info included in metrics.json for correlation with performance data
7. Version info accessible via runtime command (future): Type 'v' during execution to display version
8. Build process: CI/CD pipeline sets BUILD_DATE and GIT_COMMIT during build (GitHub Actions)
9. Development mode: If GIT_COMMIT not set, use "dev" placeholder
10. Unit tests verify: version string format, all fields populated correctly
11. Integration test: Launch with --version, verify output format matches expected
12. Manual test: Review version output, confirm all information is accurate and helpful

---

### Story 4.4: Dry-Run Mode and Configuration Validation

**As a** developer,
**I want** to validate configuration without starting the processing loop,
**so that** I can test config changes safely before deploying to production.

**Acceptance Criteria:**

1. `--dry-run` flag triggers validation-only mode (no frame processing)
2. Dry-run mode performs all startup health checks (Story 4.2)
3. Additional validation in dry-run mode:
   - **Config validation**: Display all parsed configuration values with types
   - **Model validation**: Load CoreML model, display input/output shapes
   - **Ollama model test**: Send test image to Ollama, verify response format
   - **RTSP test capture**: Capture 10 frames, display frame rate and resolution
   - **Motion detection test**: Capture 10 frames, run motion detection, display sensitivity
   - **Storage calculation**: Display current storage usage breakdown (DB, logs, images)
4. Dry-run output format:
   ```
   [DRY-RUN] Configuration Validation

   Camera Configuration:
     RTSP URL: rtsp://192.168.1.100:554/stream
     Resolution: 1920x1080 @ 30fps
     Motion threshold: 25
     Frame sampling rate: 10fps

   Model Configuration:
     CoreML model: models/yolov3.mlmodel
       Input shape: (1, 3, 416, 416)
       Output: 80 object classes
     Ollama model: llava:latest
       Context length: 2048 tokens

   Processing Configuration:
     Object blacklist: ["cat", "tree"]
     Event suppression window: 30 seconds

   Storage Configuration:
     Current usage: 1.2GB / 4GB (30%)
     Database: 450MB (15,234 events)
     Logs: 680MB
     Images: 92MB

   [TEST] Capturing test frames from camera...
   [TEST] âœ“ Captured 10 frames successfully (avg 33ms/frame)
   [TEST] Running motion detection test...
   [TEST] âœ“ Motion detected in 3/10 frames (30% sensitivity)
   [TEST] Running CoreML inference test...
   [TEST] âœ“ Object detection completed in 87ms (Neural Engine)
   [TEST] Running Ollama LLM test...
   [TEST] âœ“ LLM inference completed in 1.2s

   [DRY-RUN] âœ“ All validations passed. System ready for production.
   [DRY-RUN] Remove --dry-run flag to start processing.
   ```
5. Dry-run exits with code 0 if all validations pass, code 2 if any fail
6. Dry-run mode completes in <60 seconds (includes test inferences)
7. No events persisted during dry-run (database/logs not modified)
8. No annotated images saved during dry-run
9. Dry-run results logged to dry_run_results.json for automated validation in CI/CD
10. Unit tests verify: dry-run flag parsing, validation steps executed, no side effects
11. Integration test: Run dry-run mode end-to-end, verify no data persisted
12. Manual test: Run dry-run before production deployment, verify all checks pass

---

### Story 4.5: Signal Handling and Graceful Shutdown

**As a** developer,
**I want** robust signal handling (SIGINT, SIGTERM, SIGHUP) with graceful shutdown,
**so that** the system can be stopped safely without data loss or corrupted state (NFR7, FR28-31).

**Acceptance Criteria:**

1. Signal handler module (core/signals.py) implements SignalHandler class with register_handlers() method
2. Signals handled:
   - **SIGINT (Ctrl+C)**: Trigger graceful shutdown
   - **SIGTERM (kill)**: Trigger graceful shutdown
   - **SIGHUP**: Reload configuration without restart (hot-reload)
3. Signal handler registration during startup (after health checks, before processing loop)
4. Graceful shutdown sequence (triggered by SIGINT/SIGTERM):
   - Stop accepting new frames (close RTSP connection)
   - Finish processing current frame (if any in progress)
   - Flush all log buffers (JSON, plaintext, metrics)
   - Close database connection with final transaction commit
   - Save final metrics snapshot to metrics.json
   - Log shutdown statistics: total_runtime, frames_processed, events_created
   - Exit with code 0
5. Shutdown timeout: If shutdown doesn't complete in 10 seconds, force exit (prevents hanging)
6. Shutdown message format:
   ```
   [SIGNAL] Received SIGINT, initiating graceful shutdown...
   [SHUTDOWN] Stopping frame capture...
   [SHUTDOWN] Processing final frame...
   [SHUTDOWN] Flushing log buffers...
   [SHUTDOWN] Closing database connection...
   [SHUTDOWN] Saving final metrics...
   [SHUTDOWN] Shutdown complete.

   Session Summary:
     Runtime: 2h 34m 12s
     Frames processed: 15,234
     Motion detected: 1,823 frames (12%)
     Events created: 342
     Events suppressed: 78 (de-duplication)
     Avg CoreML inference: 89ms
     Avg LLM inference: 1.3s
     Storage used: 2.1GB / 4GB

   [EXIT] Video Recognition System stopped gracefully.
   ```
7. SIGHUP hot-reload sequence:
   - Log: "Received SIGHUP, reloading configuration..."
   - Reload config.yaml without stopping processing
   - Re-validate configuration (if invalid, keep old config and log error)
   - Apply new settings: motion_threshold, frame_sampling_rate, object_blacklist
   - Reconnect to RTSP if camera URL changed
   - Reload CoreML model if model path changed
   - Switch Ollama model if model name changed
   - Log: "Configuration reloaded successfully" or "Configuration reload failed: [reason]"
8. Signal safety: Signal handlers use thread-safe flags (threading.Event) to communicate with main loop
9. Idempotent shutdown: Multiple SIGINT signals don't cause errors (second signal logs "Shutdown already in progress...")
10. Unit tests verify: signal handler registration, shutdown sequence, hot-reload logic
11. Integration test: Send SIGINT during processing, verify graceful shutdown and data integrity
12. Manual test: Run for 1 hour, send SIGINT, verify clean shutdown and session summary

---

### Story 4.6: Runtime Performance Metrics Display

**As a** developer,
**I want** real-time performance metrics displayed during execution,
**so that** I can monitor system health and validate NFR performance targets in production (NFR21, FR25-26).

**Acceptance Criteria:**

1. Runtime metrics display implemented in core/runtime_display.py as MetricsDisplay class
2. Metrics displayed to console every 60 seconds (configurable via --metrics-interval)
3. Metrics display format:
   ```
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Runtime Metrics - 2025-11-08 14:32:15 (uptime: 2h 34m 12s)     â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ Processing:                                                      â”‚
   â”‚   Frames processed:        15,234                               â”‚
   â”‚   Motion detected:         1,823 (12% hit rate)                 â”‚
   â”‚   Events created:          342                                  â”‚
   â”‚   Events suppressed:       78 (de-duplication)                  â”‚
   â”‚                                                                  â”‚
   â”‚ Performance:                                                     â”‚
   â”‚   Frame capture:           avg 33ms, max 45ms                   â”‚
   â”‚   Motion detection:        avg 12ms, max 18ms                   â”‚
   â”‚   CoreML inference:        avg 89ms, p95 102ms, max 134ms âœ“     â”‚
   â”‚   LLM inference:           avg 1.3s, p95 1.8s, max 2.4s         â”‚
   â”‚   End-to-end latency:      avg 1.5s, p95 2.1s                   â”‚
   â”‚                                                                  â”‚
   â”‚ Resources:                                                       â”‚
   â”‚   CPU usage:               avg 45%, current 52%                 â”‚
   â”‚   Memory usage:            1.2GB / 8GB (15%)                    â”‚
   â”‚   Storage:                 2.1GB / 4GB (52%) âœ“                  â”‚
   â”‚                                                                  â”‚
   â”‚ Availability:                                                    â”‚
   â”‚   System uptime:           99.8% (5 errors in 15,234 frames)    â”‚
   â”‚   RTSP connection:         Healthy (reconnects: 0)              â”‚
   â”‚   Database status:         Healthy (15,234 events)              â”‚
   â”‚                                                                  â”‚
   â”‚ [âœ“] = Meeting NFR target  [âš ] = Approaching limit  [âœ—] = Failed â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   ```
4. Performance indicators:
   - âœ“ (green checkmark): Metric meets NFR target
   - âš  (yellow warning): Metric within 10% of NFR limit
   - âœ— (red X): Metric exceeds NFR limit (performance degraded)
5. NFR target annotations:
   - CoreML inference <100ms: Show âœ“ if <100ms, âš  if 100-110ms, âœ— if >110ms
   - Storage <4GB: Show âœ“ if <3.6GB, âš  if 3.6-4GB, âœ— if >4GB
6. Metrics display logging: Each display snapshot also logged to logs/metrics.json
7. Display refresh: Console output uses ANSI escape codes to update in place (doesn't scroll)
8. Minimal mode: If --metrics-interval=0, disable runtime display (only log to file)
9. Compact mode: First 5 minutes show full display, then switch to compact (one-line summary)
10. Metrics display overhead: <0.5% CPU (verified via profiling)
11. Unit tests verify: metric formatting, indicator logic, ANSI escape codes
12. Integration test: Run for 10 minutes, verify metrics display updates correctly
13. Manual test: Monitor display during 1-hour run, verify accuracy against metrics.json

---

### Story 4.7: Comprehensive Error Handling and User Feedback

**As a** developer,
**I want** clear error messages with actionable guidance for all failure scenarios,
**so that** users can quickly diagnose and fix issues without deep technical knowledge (NFR23, NFR28).

**Acceptance Criteria:**

1. Error handling strategy implemented across all modules with consistent format
2. Error categories and handling:
   - **Configuration errors**: Clear message + example of correct format
   - **Dependency errors**: Clear message + installation/setup instructions
   - **Runtime errors**: Clear message + suggested recovery action
   - **Network errors**: Clear message + connectivity troubleshooting steps
3. Error message format:
   ```
   [ERROR] Category: Brief description
   Details: Specific error details
   Solution: Step-by-step fix instructions
   Documentation: Link to relevant docs section
   ```
4. Example error messages:
   ```
   [ERROR] Configuration: Invalid motion_threshold value
   Details: motion_threshold must be between 0 and 255, got: 300
   Solution: Edit config.yaml and set motion_threshold to a value between 0-255
            Recommended values: 15 (high sensitivity) to 50 (low sensitivity)
   Documentation: See docs/configuration.md#motion-detection

   [ERROR] Dependency: Ollama service not running
   Details: Connection refused on http://localhost:11434
   Solution: 1. Install Ollama: brew install ollama
            2. Start service: ollama serve
            3. Verify running: curl http://localhost:11434
   Documentation: See docs/setup.md#ollama-setup

   [ERROR] Runtime: RTSP connection lost
   Details: Stream interrupted after 2h 15m (15,234 frames processed)
   Solution: System will attempt reconnection in 5 seconds...
            If connection fails 3 times, check camera status and network
   Documentation: See docs/troubleshooting.md#rtsp-connection-issues
   ```
5. Error logging: All errors logged at ERROR level with full stack trace in logs/error.log
6. User-facing errors: Console output shows simplified message (no stack trace)
7. Error recovery strategies:
   - **Transient errors**: Auto-retry with exponential backoff (RTSP reconnection)
   - **Configuration errors**: Exit immediately with error code 2
   - **Fatal errors**: Graceful shutdown with error code 1
   - **Storage errors**: Trigger log rotation, then retry
8. Error aggregation: Similar errors suppressed (log "RTSP reconnection failed (3 attempts)" instead of 3 separate messages)
9. Exit codes documented in README:
   - 0: Clean shutdown
   - 1: Fatal error (runtime failure)
   - 2: Configuration invalid
   - 3: Storage limit exceeded
   - 4: Dependency missing/incompatible
10. Unit tests verify: error message formatting, exit codes, recovery logic
11. Integration test: Trigger each error category, verify message quality and exit code
12. Manual test: Review all error messages for clarity and actionable guidance

---

### Story 4.8: Platform Compatibility Checks

**As a** developer,
**I want** explicit platform compatibility validation (macOS, Apple Silicon, Python version),
**so that** users get clear feedback if running on unsupported platforms (NFR1, NFR24-25).

**Acceptance Criteria:**

1. Platform check module (core/platform_check.py) implements PlatformValidator class
2. Platform validation during startup health check (before model loading):
   - **OS check**: Verify platform.system() == "Darwin" (macOS)
   - **Architecture check**: Verify platform.machine() == "arm64" (Apple Silicon)
   - **Chip check**: Detect M1/M2/M3 via sysctl hw.model
   - **Python version**: Verify sys.version_info >= (3, 10)
   - **macOS version**: Verify macOS >= 13.0 (Ventura) for CoreML Neural Engine support
3. Platform check results displayed during startup:
   ```
   [PLATFORM] âœ“ macOS 14.2 (23C64)
   [PLATFORM] âœ“ Apple M1 Pro (arm64)
   [PLATFORM] âœ“ Python 3.10.12
   [PLATFORM] âœ“ CoreML Neural Engine supported
   ```
4. Platform validation failures with specific error messages:
   ```
   [ERROR] Platform: Unsupported operating system
   Details: This system requires macOS 13.0 or later, detected: Linux
   Solution: This application is designed for Apple Silicon Macs (M1/M2/M3)
            For other platforms, see docs/alternative-platforms.md

   [ERROR] Platform: Unsupported architecture
   Details: Apple Silicon (M1/M2/M3) required, detected: x86_64
   Solution: This application requires Apple Neural Engine for CoreML inference
            Intel Macs are not supported (no Neural Engine)

   [ERROR] Platform: Python version too old
   Details: Python 3.10 or later required, detected: 3.9.7
   Solution: Upgrade Python: brew install python@3.10
            Then recreate virtual environment with Python 3.10
   ```
5. Warning for unsupported configurations (don't exit, but warn):
   - macOS <14.0: "Warning: macOS 13.x supported but macOS 14+ recommended for best performance"
   - Python 3.13+: "Warning: Tested on Python 3.10-3.12, Python 3.13 may have compatibility issues"
6. Neural Engine detection: Use coremltools to verify Neural Engine availability
7. Platform info logged at startup for diagnostics (chip model, RAM, CPU cores)
8. Partial support mode (future): If platform check fails, suggest alternative config (e.g., CPU inference instead of Neural Engine)
9. CI/CD integration: Platform check results included in automated test reports
10. Unit tests verify: platform detection logic, error messages for each unsupported platform
11. Integration test: Mock platform values, verify correct validation and error messages
12. Manual test: Attempt to run on Intel Mac, verify clear error message (if available for testing)

---

### Story 4.9: Documentation and Example Configuration

**As a** developer,
**I want** comprehensive README, example configuration, and troubleshooting guide,
**so that** new users can set up and run the system without external help (NFR15-16, NFR28).

**Acceptance Criteria:**

1. README.md created with following sections (NFR16):
   - **Overview**: 2-3 sentence description of what the system does
   - **Features**: Bullet list of key capabilities (motion detection, CoreML, Ollama, etc.)
   - **Requirements**: Platform requirements (macOS 13+, M1/M2/M3, Python 3.10+)
   - **Installation**: Step-by-step setup instructions (clone, venv, requirements, models)
   - **Quick Start**: Minimal steps to run for first time (config setup, dry-run, run)
   - **Configuration**: Table of all config parameters with descriptions and defaults
   - **Usage**: CLI options and examples (--dry-run, --config, --version)
   - **Architecture**: High-level diagram showing processing pipeline flow
   - **Performance**: NFR targets and expected metrics on reference hardware
   - **Troubleshooting**: â‰¥5 common issues with solutions (as per NFR16)
   - **FAQ**: â‰¥5 frequently asked questions (as per NFR16)
   - **Development**: How to run tests, contribute, code structure
   - **License**: MIT license
2. Example configuration file created: config/config.example.yaml
   ```yaml
   # Example configuration for Local Video Recognition System
   # Copy to config.yaml and customize for your setup

   camera:
     rtsp_url: "rtsp://192.168.1.100:554/stream"
     username: "admin"
     password: "password"
     # Optional: camera identifier for multi-camera setups
     camera_id: "front-door"

   processing:
     # Motion detection threshold (0-255, lower = more sensitive)
     motion_threshold: 25
     # Frame sampling rate (fps) for motion detection
     frame_sampling_rate: 10
     # Objects to ignore (won't trigger events)
     object_blacklist: ["cat", "tree", "car"]
     # Event de-duplication window (seconds)
     event_suppression_window: 30

   models:
     # Path to CoreML model file
     coreml_model: "models/yolov3.mlmodel"
     # Ollama model name (must be pulled first: ollama pull llava:latest)
     ollama_model: "llava:latest"
     # Minimum confidence for object detection (0.0-1.0)
     confidence_threshold: 0.5

   storage:
     # Maximum storage for events/logs/images (GB)
     max_storage_gb: 4
     # Minimum retention period (days)
     min_retention_days: 7
     # Check storage every N events
     storage_check_interval: 100

   logging:
     # Logging level (DEBUG, INFO, WARNING, ERROR)
     log_level: "INFO"
     # Metrics display interval (seconds, 0 to disable)
     metrics_interval: 60
   ```
3. Troubleshooting guide (docs/troubleshooting.md) with â‰¥10 common issues:
   - Ollama service not running
   - RTSP authentication failed
   - CoreML model not found
   - Neural Engine not available
   - Storage limit exceeded
   - High CPU usage
   - High latency (>3s)
   - Camera connection keeps dropping
   - Configuration validation errors
   - Permission denied errors
4. Architecture diagram (docs/architecture.md) showing:
   - RTSP camera input
   - Motion detection stage
   - CoreML object detection
   - Ollama LLM inference
   - SQLite persistence
   - File logging (JSON + plaintext)
   - Storage management
5. Configuration reference (docs/configuration.md) documenting all 20+ config parameters with:
   - Parameter name
   - Data type
   - Default value
   - Valid range/options
   - Description
   - Example usage
   - Related NFRs
6. Setup script (scripts/setup.sh) automating initial setup:
   ```bash
   #!/bin/bash
   # Setup script for Local Video Recognition System
   python3 -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   mkdir -p data/events logs config models
   cp config/config.example.yaml config/config.yaml
   echo "Setup complete. Edit config/config.yaml and run: python main.py --dry-run"
   ```
7. Model download script (scripts/download_models.sh) for CoreML models
8. FAQ section includes:
   - What hardware do I need?
   - How much storage does it use?
   - Can I use multiple cameras?
   - What RTSP cameras are supported?
   - How do I improve detection accuracy?
   - Can I run this on Intel Mac? (No, requires Apple Silicon)
   - How do I update Ollama models?
9. README includes badges: Python version, license, platform support
10. Documentation tested: Follow README start-to-finish on fresh Mac, verify works
11. Unit tests verify: example config is valid (can be parsed)
12. Manual test: Ask colleague unfamiliar with project to set up using only README

---

**Epic 4 Summary:**
- **9 Stories** delivering production-ready CLI interface, robust signal handling, comprehensive error management, and complete documentation
- **Vertical slices:** Each story delivers complete operational capability
- **Core value delivered:** System is professional, user-friendly, and ready for 24/7 deployment
- **Deployable:** After Epic 4, complete MVP ready for production use on Mac Mini/Studio with full documentation

---

### Rationale for Story Breakdown:

**Why 9 stories:**
- CLI foundation (4.1): Entry point and argument parsing
- Validation (4.2-4.4): 3 stories for health checks, version info, dry-run mode
- Operations (4.5-4.6): 2 stories for signal handling and runtime metrics
- Quality (4.7-4.8): 2 stories for error handling and platform checks
- Documentation (4.9): 1 story for README and guides

**Story sequencing:**
- 4.1 (CLI) first - entry point needed before other features
- 4.2 (Health checks) before 4.4 (Dry-run) - dry-run uses health checks
- 4.5 (Signals) and 4.6 (Metrics) parallel - independent concerns
- 4.7 (Errors) and 4.8 (Platform) parallel - both improve reliability
- 4.9 (Docs) last - can document complete system

**Cross-cutting concerns:**
- User experience: Clear messages, helpful guidance, professional polish
- Reliability: Graceful shutdown, error recovery, platform validation
- Operations: Health checks, metrics, monitoring, troubleshooting
- Documentation: README requirements (NFR16), setup instructions, examples

---

## Checklist Results Report

### Executive Summary

**Overall PRD Completeness:** 98% (improved from 95% after adding schema migration strategy)

**MVP Scope Appropriateness:** Just Right - Focused on core value delivery with clear phase separation

**Readiness for Architecture Phase:** Ready - All HIGH priority items addressed

**Remaining Minor Gaps:**
- No user personas defined (acceptable for personal learning project)
- Market analysis/competitive analysis minimal (acceptable for personal project scope)
- Architecture diagram recommended (already planned in Story 4.9)

### Category Analysis

| Category                         | Status  | Critical Issues                                                                          |
| -------------------------------- | ------- | ---------------------------------------------------------------------------------------- |
| 1. Problem Definition & Context  | PASS    | Clear problem statement, target user (developer), success metrics well-defined           |
| 2. MVP Scope Definition          | PASS    | Excellent scope boundaries, Phase 1/2 separation, clear out-of-scope items               |
| 3. User Experience Requirements  | PASS    | CLI for Phase 1 well-defined, Web UI vision documented for Phase 2                       |
| 4. Functional Requirements       | PASS    | 35 FRs organized logically, all testable, good traceability to stories                   |
| 5. Non-Functional Requirements   | PASS    | 29 NFRs with quantitative targets, comprehensive test plan created                       |
| 6. Epic & Story Structure        | PASS    | 4 epics, 35 stories total, excellent sequencing, appropriate AI agent sizing             |
| 7. Technical Guidance            | PASS    | Comprehensive technical assumptions, clear constraints for architect                     |
| 8. Cross-Functional Requirements | PASS    | Schema migration strategy added, all data/integration/operational requirements complete  |
| 9. Clarity & Communication       | PASS    | Excellent documentation quality, clear language, good use of examples and code snippets  |

### Detailed Analysis by Category

#### 1. Problem Definition & Context âœ“ PASS (95%)
**Strengths:**
- âœ“ Clear problem: Commercial cloud costs, privacy concerns, latency for local security cameras
- âœ“ Target user identified: Developer with Apple Silicon Mac learning ML/CV
- âœ“ Quantifiable success metrics: <100ms CoreML inference, <4GB storage, â‰¥70% test coverage
- âœ“ Differentiation from cloud solutions well-articulated (privacy-first, 100% local)
- âœ“ Business value for learning laboratory clearly stated

**Minor Gaps:**
- Formal user personas not defined (acceptable for personal project)
- No competitive landscape analysis (acceptable given unique approach)

**Assessment:** Problem-solution fit is excellent. The "learning laboratory" goal justifies the technical complexity.

#### 2. MVP Scope Definition âœ“ PASS (98%)
**Strengths:**
- âœ“ Excellent core functionality focus: Motion â†’ CoreML â†’ Ollama â†’ Persistence â†’ CLI
- âœ“ Clear Phase 1 (MVP) vs Phase 2 (Future) separation
- âœ“ Out-of-scope items explicitly documented (multi-camera, cloud upload, mobile app)
- âœ“ Each epic delivers incremental, deployable value
- âœ“ MVP validation approach clear: Deploy on Mac Mini/Studio for 24/7 operation
- âœ“ Rationale for technical choices well-documented

**Minor Gaps:**
- Could specify expected timeline/duration for MVP completion (minor)

**Assessment:** This is a textbook example of properly scoped MVP. Not too ambitious, focuses on core value.

#### 3. User Experience Requirements âœ“ PASS (92%)
**Strengths:**
- âœ“ Phase 1 CLI user flows well-defined (startup â†’ health checks â†’ processing â†’ shutdown)
- âœ“ Entry/exit points clear (--help, --dry-run, SIGINT shutdown)
- âœ“ Error states and recovery planned (Story 4.7: Comprehensive Error Handling)
- âœ“ Performance expectations from user perspective (startup <30s, metrics every 60s)
- âœ“ Accessibility N/A for CLI (appropriate)
- âœ“ Phase 2 web UI vision documented (5 core screens, dark mode, 4K optimized)

**Minor Gaps:**
- Primary user flows could be visualized in a sequence diagram (enhancement)
- Web UI wireframes deferred to UX Expert (appropriate)

**Assessment:** UX requirements appropriate for CLI MVP with good vision for Phase 2 web dashboard.

#### 4. Functional Requirements âœ“ PASS (97%)
**Strengths:**
- âœ“ 35 Functional Requirements organized in 10 logical categories
- âœ“ All requirements testable and verifiable
- âœ“ Requirements focus on WHAT not HOW (proper abstraction)
- âœ“ Consistent terminology throughout (RTSP, CoreML, Ollama, SQLite, YAML)
- âœ“ Complex features broken down (e.g., FR11-14 for logging covers JSON, plaintext, metrics, rotation)
- âœ“ Dependencies explicit (FR21-24 startup must complete before FR25-27 runtime)
- âœ“ Excellent traceability: Requirements map clearly to epic stories
- âœ“ User-focused descriptions with clear rationale

**Assessment:** Functional requirements are comprehensive, well-structured, and ready for implementation.

#### 5. Non-Functional Requirements âœ“ PASS (100%)
**Strengths:**
- âœ“ 29 Non-Functional Requirements with quantitative targets
- âœ“ Performance: CoreML <100ms, startup <30s, event processing <3s
- âœ“ Resource constraints: <4GB storage, memory efficient
- âœ“ Reliability: Graceful degradation, error recovery, 24/7 operation
- âœ“ Platform: macOS 13+, Apple Silicon, Python 3.10+
- âœ“ Security: Local-only processing, no cloud, credentials in YAML
- âœ“ Testing: â‰¥70% code coverage, comprehensive test plan document created
- âœ“ All NFRs have verification methods in test plan
- âœ“ NFR testability: Every NFR is measurable and verifiable

**Assessment:** This is exceptional NFR quality. All requirements have quantitative targets and test methods.

#### 6. Epic & Story Structure âœ“ PASS (98%)
**Strengths:**
- âœ“ 4 Epics representing cohesive value delivery units
- âœ“ Epic sequence logical: Foundation â†’ Intelligence â†’ Persistence â†’ Production
- âœ“ Each epic deployable and delivers tangible value
- âœ“ 35 total stories across 4 epics (8-9 stories per epic)
- âœ“ Stories follow consistent format: "As a/I want/So that" with 10-12 acceptance criteria
- âœ“ Stories sized for AI agent execution (2-4 hours each)
- âœ“ Vertical slices: Each story delivers complete functionality
- âœ“ Dependencies explicit and properly sequenced
- âœ“ First epic includes all foundation work (project setup, config, health checks)
- âœ“ Cross-cutting concerns distributed (logging in Epic 1, metrics in Epic 3, error handling in Epic 4)
- âœ“ Acceptance criteria are testable, specific, and comprehensive

**Minor Gaps:**
- Could add estimated story points (optional, not critical for AI agent execution)

**Assessment:** Epic and story structure is exemplary. Ready for development without further breakdown.

#### 7. Technical Guidance âœ“ PASS (96%)
**Strengths:**
- âœ“ Comprehensive technical assumptions documented
- âœ“ Repository structure: Monorepo with clear separation (core/, platform/, integrations/, api/, web/)
- âœ“ Service architecture: Monolith â†’ Modular Monolith â†’ Future microservices evolution
- âœ“ Programming languages specified: Python 3.10+, FastAPI, Vanilla JS
- âœ“ Data storage decisions: SQLite, YAML, JSON+plaintext dual logging
- âœ“ Key libraries pinned: OpenCV 4.8+, coremltools 7.0+, ollama-python 0.1.0+
- âœ“ Testing strategy: pytest with â‰¥70% coverage, GitHub Actions CI/CD
- âœ“ Deployment targets: macOS 13+ M1 MacBook Pro (dev), Mac Mini/Studio (prod)
- âœ“ Performance constraints guide architecture (Neural Engine, <100ms inference)
- âœ“ Security requirements clear (local-only, no auth initially)
- âœ“ Example YAML config provided for architect reference

**Minor Gaps:**
- Could specify branching strategy (git flow, trunk-based) - minor
- Could add guidelines on when to refactor from monolith to modular (enhancement)

**Assessment:** Technical guidance is thorough and provides clear constraints for architect.

#### 8. Cross-Functional Requirements âœ“ PASS (95%)
**Strengths:**
- âœ“ Data entities identified: events table with 10 fields
- âœ“ Data storage requirements: SQLite for events, file-based for logs
- âœ“ Data retention policies: 4GB limit, 7 day minimum, FIFO rotation
- âœ“ Schema provided in Story 3.1 with CREATE TABLE statement
- âœ“ Schema evolution strategy: Migration scripts in migrations/ directory with schema_version tracking (Story 3.1 criterion #8)
- âœ“ Integration requirements: Ollama HTTP API, RTSP camera protocol
- âœ“ API requirements for Phase 2 planned (FastAPI)
- âœ“ Operational requirements: 24/7 deployment, signal handling, graceful shutdown
- âœ“ Monitoring approach: metrics.json logging, runtime display, storage checks

**Minor Gaps:**
- Backup/restore strategy could be more detailed (sqlite3 .dump script documented in Story 3.1 criterion #13)
- Logging rotation strategy could specify log levels per component (enhancement, not critical)

**Assessment:** Cross-functional requirements are comprehensive and production-ready.

#### 9. Clarity & Communication âœ“ PASS (98%)
**Strengths:**
- âœ“ Document uses clear, consistent language throughout
- âœ“ Excellent structure: Goals â†’ Requirements â†’ UI â†’ Technical â†’ Epics â†’ Stories
- âœ“ Technical terms defined in context (RTSP, CoreML, Neural Engine, Ollama)
- âœ“ Code examples provided (SQL schema, YAML config, error messages, CLI output)
- âœ“ Rationale documented for key decisions (4 epics vs 3, story sequencing)
- âœ“ Change log table included (Version 1.0 initial draft)
- âœ“ Consistent formatting: FR/NFR numbering, story numbering (Epic.Story)
- âœ“ Examples throughout enhance understanding (config YAML, health check output, metrics display)

**Minor Gaps:**
- Could add architecture diagram for visual learners (recommended for Story 4.9)
- Could add glossary section for acronyms (RTSP, LLM, CV, ML)

**Assessment:** Documentation quality is exceptional. Ready for stakeholder review.

### Top Issues by Priority

#### BLOCKERS
None identified. PRD is ready for architecture phase.

#### HIGH PRIORITY (Should Address)
1. **Schema Evolution Strategy** - âœ… **COMPLETED** - Added to Story 3.1 acceptance criterion #8
   - **Implementation:** Manual migration scripts in migrations/ directory with schema_version tracking table
   - **Details:** On startup, system checks schema version and applies pending migrations in sequence
   - **Fallback:** For MVP Phase 1, documented alternative is backup â†’ delete â†’ recreate (acceptable for single-user system)

#### MEDIUM PRIORITY (Would Improve Clarity)
1. **Architecture Diagram** - Visual representation of processing pipeline
   - **Recommendation:** Add to Story 4.9 (Documentation) - "Create docs/architecture.md with pipeline flow diagram"
   - **Impact:** Low - Helpful but PRD text is already clear

2. **Timeline Expectations** - Approximate duration for MVP completion
   - **Recommendation:** Add to Goals section: "Expected timeline: 4-6 weeks (35 stories Ã— 2-4 hours each = 70-140 AI agent hours)"
   - **Impact:** Low - Helps set expectations

#### LOW PRIORITY (Nice to Have)
1. **Glossary Section** - Define all acronyms (RTSP, CV, ML, LLM, etc.)
2. **Story Points** - Add optional t-shirt sizing (S/M/L) to stories
3. **Branching Strategy** - Specify git workflow (trunk-based development recommended)

### MVP Scope Assessment

**Features that might be cut for faster MVP:**
- Event de-duplication (FR9, Story 2.8) - Could defer to v1.1
- Plaintext logging (FR13, Story 3.5) - JSON logs might be sufficient
- Hot-reload on SIGHUP (Story 4.5) - Could require full restart initially

**Assessment:** Current MVP scope is lean. Above cuts would save ~3 stories but reduce quality. **Recommendation: Keep current scope.**

**Missing features that are essential:**
None identified. All critical functionality is present.

**Complexity concerns:**
- Ollama LLM integration (Epic 2) has highest uncertainty - requires Ollama service running
- RTSP camera compatibility may vary by manufacturer
- CoreML model selection/conversion not fully specified

**Mitigations already in place:**
- âœ“ Health checks validate Ollama availability (Story 4.2)
- âœ“ Dry-run mode tests RTSP connectivity (Story 4.4)
- âœ“ Model download script planned (Story 4.9)

**Timeline realism:**
- 35 stories Ã— 3 hours average = 105 AI agent hours
- Assuming 50% efficiency (debugging, testing, refinement) = ~210 hours
- At 8 hours/day = 26 working days (~5 weeks)
- **Assessment:** Timeline is realistic for AI-assisted development

### Technical Readiness

**Clarity of technical constraints:**
- âœ“ Excellent - All constraints documented (platform, libraries, performance targets)
- âœ“ No ambiguity on technology choices (Python, SQLite, FastAPI, etc.)
- âœ“ Architect has clear guidance on what NOT to do (no cloud, no microservices for MVP)

**Identified technical risks:**
1. **CoreML model availability** - Need to source or convert YOLO model to CoreML
   - Mitigation: Story 4.9 includes model download script
2. **Ollama performance** - LLM inference may exceed targets on M1 base
   - Mitigation: NFR allows up to 3s for LLM (pragmatic target)
3. **RTSP camera compatibility** - Protocol variations across manufacturers
   - Mitigation: Dry-run mode tests connectivity before production

**Areas needing architect investigation:**
1. **CoreML model selection** - YOLO vs MobileNet vs custom model for best Neural Engine performance
2. **Ollama model choice** - LLaVA vs Moondream for speed/accuracy tradeoff
3. **Concurrency strategy** - Threading vs asyncio for frame processing pipeline
4. **Database connection pooling** - SQLite locking in high-frequency writes

**Assessment:** Technical risks are identified and mitigated. Architect has clear investigation areas.

### Recommendations

#### For Product Manager (Before Handoff)
1. **HIGH:** âœ… Schema migration strategy added to Story 3.1 acceptance criterion #8
2. **MEDIUM:** Add approximate timeline to Goals section (4-6 weeks) - OPTIONAL
3. **MEDIUM:** Ensure docs/architecture.md includes pipeline flow diagram (already in Story 4.9) - âœ… COVERED
4. **LOW:** Consider adding glossary to README (already in Story 4.9 scope) - âœ… COVERED

#### For Architect
1. **HIGH:** Investigate CoreML model options (YOLO, MobileNet, etc.) - create model performance comparison
2. **HIGH:** Design concurrency strategy (threading vs asyncio) - evaluate tradeoffs
3. **MEDIUM:** Evaluate Ollama model options (LLaVA vs Moondream) - benchmark inference times
4. **MEDIUM:** Design SQLite connection management strategy for concurrent writes
5. **LOW:** Consider schema versioning approach (Alembic or manual migrations)

#### For UX Expert (Phase 2 Prep)
1. Create wireframes for 5 core web dashboard screens
2. Design event timeline visualization with multi-camera correlation
3. Design web-based YAML editor with validation feedback
4. Create dark mode color palette optimized for 4K displays

### Final Decision

**âœ… READY FOR ARCHITECT**

The PRD and epics are comprehensive, properly structured, and ready for architectural design. The requirements documentation is excellent with:

- Clear problem definition and MVP scope
- Comprehensive functional and non-functional requirements (64 total)
- Well-structured epic breakdown (4 epics, 35 stories)
- Detailed acceptance criteria (10-12 per story)
- Thorough technical guidance and constraints
- Excellent documentation quality and clarity

**Confidence Level:** 98% - This PRD provides everything the architect needs to proceed

**All Critical Items Addressed:**
- âœ… Schema migration strategy added (Story 3.1 criterion #8)
- âœ… Architecture diagram planned (Story 4.9)
- âœ… All functional and non-functional requirements complete

**Recommended Next Steps:**
1. Hand off to Architect for technical design document creation
2. Architect should focus investigation on: CoreML model selection, concurrency strategy, Ollama model benchmarking
3. Optional: Add timeline estimate to Goals section (low priority)

**Strengths of This PRD:**
- Exemplary NFR quality (all quantitative, all testable)
- Excellent story sizing for AI agent execution (2-4 hours each)
- Strong technical assumptions section (clear constraints)
- Comprehensive test plan document (27 test cases, all NFRs covered)
- Pragmatic scope (focused MVP with clear Phase 2 vision)

---

## Next Steps

### UX Expert Prompt

**Note:** UX work is planned for Phase 2 (Web Dashboard). For Phase 1 MVP, CLI interface is well-specified in Epic 4. When ready for Phase 2:

```
Create web dashboard UI specification for Local Video Recognition System based on PRD docs/prd.md.

Focus on:
- 5 core screens: Dashboard, Event Timeline, Event Detail, Settings, System Status
- Multi-camera event correlation visualization
- Web-based YAML configuration editor with validation
- Performance metrics display (frame processing time, latency, availability)
- Dark mode optimized for 4K desktop displays
- No authentication (local-only access)

Reference PRD Section "User Interface Design Goals" for detailed requirements.
```

### Architect Prompt

```
Create technical architecture document for Local Video Recognition System based on PRD docs/prd.md.

Key areas for architectural investigation:
1. CoreML model selection (YOLO vs MobileNet vs others) - optimize for Apple Neural Engine <100ms inference
2. Concurrency strategy (threading vs asyncio) for processing pipeline (motion detection â†’ CoreML â†’ Ollama â†’ persistence)
3. Ollama model benchmarking (LLaVA vs Moondream) - balance speed vs accuracy for semantic descriptions
4. SQLite connection management for concurrent writes (event persistence during high-frequency motion events)
5. Schema versioning and migration strategy for database evolution

Technical constraints are documented in PRD Section "Technical Assumptions". All technology choices are already defined (Python 3.10+, SQLite, FastAPI, OpenCV, etc.).

Focus on:
- Processing pipeline architecture (stages, data flow, error handling)
- Module structure following monorepo layout (core/, platform/, integrations/)
- Class hierarchy and interfaces for extensibility
- Performance optimization strategies to meet NFR targets
- Testing architecture to achieve â‰¥70% coverage

Deliverable: docs/architecture.md following architecture template.
```

---
