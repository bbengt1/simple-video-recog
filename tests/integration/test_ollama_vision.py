"""Integration tests for Ollama vision LLM functionality."""

import numpy as np
import pytest
from pydantic import HttpUrl

from core.config import SystemConfig
from core.exceptions import OllamaConnectionError, OllamaTimeoutError
from core.models import DetectionResult, DetectedObject, BoundingBox
from integrations.ollama import OllamaClient


@pytest.fixture
def vision_config():
    """Create configuration for vision LLM testing."""
    return SystemConfig(
        camera_rtsp_url="rtsp://test:stream@localhost:8554/test",
        ollama_base_url=HttpUrl("http://localhost:11434"),
        ollama_model="llava:7b",
        llm_timeout=10
    )


@pytest.fixture
def sample_frame():
    """Create sample frame for vision testing."""
    # Create a simple colored frame (480x640 RGB)
    frame = np.zeros((480, 640, 3), dtype=np.uint8)
    # Add some color variation to make it more realistic
    frame[100:200, 200:400] = [255, 0, 0]  # Red rectangle
    frame[250:350, 100:300] = [0, 255, 0]  # Green rectangle
    return frame


@pytest.fixture
def sample_detections():
    """Create sample detections for testing."""
    objects = [
        DetectedObject(
            label="person",
            confidence=0.95,
            bbox=BoundingBox(x=200, y=100, width=200, height=300)
        ),
        DetectedObject(
            label="car",
            confidence=0.87,
            bbox=BoundingBox(x=100, y=250, width=200, height=100)
        )
    ]
    return DetectionResult(
        objects=objects,
        inference_time=0.05,
        frame_shape=(480, 640, 3)
    )


class TestOllamaVisionIntegration:
    """Integration tests for Ollama vision functionality.

    These tests require Ollama to be running locally with llava:7b model.
    Tests will be skipped gracefully if Ollama is not available.
    """

    @pytest.fixture(autouse=True)
    def setup_method(self, vision_config):
        """Set up test method with Ollama client."""
        self.client = OllamaClient(vision_config)
        self.ollama_available = self._check_ollama_available()

    def _check_ollama_available(self):
        """Check if Ollama service is available."""
        try:
            self.client.connect()
            self.client.verify_model(self.client.config.ollama_model)
            return True
        except (OllamaConnectionError, Exception):
            return False

    @pytest.mark.skipif("not TestOllamaVisionIntegration()._check_ollama_available()",
                       reason="Ollama service not available")
    def test_generate_description_with_real_ollama(self, sample_frame, sample_detections, caplog):
        """Test real vision description generation with Ollama."""
        with caplog.at_level("INFO"):
            description = self.client.generate_description(sample_frame, sample_detections)

        # Verify we got a non-empty description
        assert isinstance(description, str)
        assert len(description.strip()) > 0

        # Verify logging includes timing information
        log_messages = [record.message for record in caplog.records]
        timing_logs = [msg for msg in log_messages if "LLM description generated" in msg and "in" in msg]
        assert len(timing_logs) > 0, "Should log timing information"

        # Verify timing is reasonable (not negative or impossibly fast)
        timing_log = timing_logs[0]
        # Extract timing value from log like "âœ“ LLM description generated (42 chars) in 2.34s"
        import re
        timing_match = re.search(r'in ([\d.]+)s', timing_log)
        assert timing_match, f"Could not extract timing from log: {timing_log}"
        timing = float(timing_match.group(1))
        assert timing > 0, "Timing should be positive"
        assert timing < 300, "Timing should be reasonable (< 5 minutes)"

    @pytest.mark.skipif("not TestOllamaVisionIntegration()._check_ollama_available()",
                       reason="Ollama service not available")
    def test_generate_description_multiple_frames(self, sample_frame, sample_detections):
        """Test generating descriptions for multiple frames."""
        frames_and_detections = []

        # Create variations of the sample data
        for i in range(3):
            # Slightly modify the frame
            modified_frame = sample_frame.copy()
            modified_frame[i*50:(i+1)*50, i*50:(i+1)*50] = [i*50, i*50, i*50]

            # Modify detections
            modified_objects = [
                DetectedObject(
                    label="person",
                    confidence=max(0.5, 0.95 - i*0.1),
                    bbox=BoundingBox(x=200+i*10, y=100+i*10, width=200, height=300)
                )
            ]
            modified_detections = DetectionResult(
                objects=modified_objects,
                inference_time=0.05 + i*0.01,
                frame_shape=(480, 640, 3)
            )

            frames_and_detections.append((modified_frame, modified_detections))

        # Generate descriptions for all frames
        descriptions = []
        for frame, detections in frames_and_detections:
            description = self.client.generate_description(frame, detections)
            descriptions.append(description)

            # Verify each description is valid
            assert isinstance(description, str)
            assert len(description.strip()) > 0

        # Verify we got different descriptions (vision model should respond differently)
        # Note: This might not always be true, but it's a reasonable expectation
        unique_descriptions = set(descriptions)
        assert len(unique_descriptions) >= 1, "Should generate at least one valid description"

    @pytest.mark.skipif("not TestOllamaVisionIntegration()._check_ollama_available()",
                       reason="Ollama service not available")
    def test_generate_description_semantic_accuracy(self, sample_frame, sample_detections):
        """Test that generated descriptions are semantically relevant."""
        description = self.client.generate_description(sample_frame, sample_detections)

        # Convert to lowercase for case-insensitive checking
        desc_lower = description.lower()

        # The description should be in English and contain some descriptive content
        assert len(desc_lower.split()) >= 3, "Description should contain multiple words"

        # Should not contain obvious error messages or API artifacts
        error_indicators = ["error", "failed", "timeout", "exception", "ollama"]
        for indicator in error_indicators:
            assert indicator not in desc_lower, f"Description should not contain error indicator: {indicator}"

    def test_ollama_not_available_graceful_skip(self, sample_frame, sample_detections):
        """Test that tests are skipped gracefully when Ollama is not available."""
        if self.ollama_available:
            pytest.skip("Ollama is available, skipping unavailability test")

        # This test should only run when Ollama is not available
        with pytest.raises((OllamaConnectionError, Exception)):
            self.client.generate_description(sample_frame, sample_detections)

    @pytest.mark.skipif("not TestOllamaVisionIntegration()._check_ollama_available()",
                       reason="Ollama service not available")
    def test_performance_within_target(self, sample_frame, sample_detections, caplog):
        """Test that LLM inference meets performance target (< 5 seconds 95th percentile)."""
        import time

        # Run multiple inferences to get performance distribution
        inference_times = []
        num_iterations = 5  # Small sample for integration testing

        for i in range(num_iterations):
            start_time = time.perf_counter()
            try:
                self.client.generate_description(sample_frame, sample_detections)
                end_time = time.perf_counter()
                inference_times.append(end_time - start_time)
            except Exception:
                # Skip failed inferences for performance testing
                continue

        if len(inference_times) == 0:
            pytest.skip("No successful inferences to measure performance")

        # Calculate 95th percentile (sort and take ~95th position)
        sorted_times = sorted(inference_times)
        percentile_95_index = int(len(sorted_times) * 0.95)
        if percentile_95_index >= len(sorted_times):
            percentile_95_index = len(sorted_times) - 1

        percentile_95 = sorted_times[percentile_95_index]

        # Log performance results
        print(f"Performance test results: {len(inference_times)} inferences")
        print(f"95th percentile: {percentile_95:.2f}s")
        print(f"Individual times: {[f'{t:.2f}s' for t in inference_times]}")

        # Assert performance target (allowing some tolerance for integration testing)
        # Target is < 5 seconds for 95th percentile
        assert percentile_95 < 10.0, f"95th percentile {percentile_95:.2f}s exceeds reasonable limit (10s for integration test)"

        # Log warning in test output if approaching target
        if percentile_95 > 3.0:
            print(f"WARNING: 95th percentile {percentile_95:.2f}s approaching 5s target")